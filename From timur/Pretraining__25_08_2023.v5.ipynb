{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03477f44-46c2-4a22-aaee-a67878c1816e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T15:37:45.921389600Z",
     "start_time": "2023-08-26T15:37:45.707650700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Aug 26 18:37:45 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.78.01    Driver Version: 525.78.01    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:19:00.0 Off |                  N/A |\r\n",
      "| 30%   32C    P8    30W / 350W |      6MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\r\n",
      "| 30%   33C    P8    21W / 350W |      6MiB / 24576MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1704      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "|    1   N/A  N/A      1704      G   /usr/lib/xorg/Xorg                  4MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e513c8-c521-4ac4-b93c-bea7dbb4eac9",
   "metadata": {},
   "source": [
    "##### %config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de61d6ab-33e9-40ef-b448-6686a5a4e0ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:47.973941Z",
     "start_time": "2023-09-01T07:06:47.967961Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# os.environ['http_proxy'] = \"http://127.0.0.1:3128\"\n",
    "# os.environ['https_proxy'] = \"http://127.0.0.1:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ab062da-1f4d-4f54-965f-49e519bb897c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:48.422759Z",
     "start_time": "2023-09-01T07:06:48.404818900Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a029d2ef-c060-410a-9c98-aa48d86c7926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:48.648323700Z",
     "start_time": "2023-09-01T07:06:48.640350400Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e1e8bc-99d9-4968-9956-fe13fc4433e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:48.841890Z",
     "start_time": "2023-09-01T07:06:48.798272200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c443a-6995-43ca-ac1d-43cfe5ec376a",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef112d9-8b15-407a-b735-cc584072cc9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:49.553273Z",
     "start_time": "2023-09-01T07:06:49.541308100Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransposeCustom(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransposeCustom, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "411dce92-23fd-4258-ab4c-73b9f68f8d5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:49.862605800Z",
     "start_time": "2023-09-01T07:06:49.858662400Z"
    }
   },
   "outputs": [],
   "source": [
    "# config = BertConfig(is_decoder=True, \n",
    "#                     add_cross_attention=True,\n",
    "#                     ff_layer='conv',\n",
    "#                     conv_kernel=1,\n",
    "#                     conv_kernel_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711ae813-492d-4fe1-89ee-8315cb43617f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:50.228479500Z",
     "start_time": "2023-09-01T07:06:50.221221600Z"
    }
   },
   "outputs": [],
   "source": [
    "def _make_span_from_seeds(seeds, span, total=None):\n",
    "    inds = list()\n",
    "    for seed in seeds:\n",
    "        for i in range(seed, seed + span):\n",
    "            if total is not None and i >= total:\n",
    "                break\n",
    "            elif i not in inds:\n",
    "                inds.append(int(i))\n",
    "    return np.array(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d78b6b1-0a91-4706-92ca-f52a791ea753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:50.473569100Z",
     "start_time": "2023-09-01T07:06:50.435144400Z"
    }
   },
   "outputs": [],
   "source": [
    "def _make_mask(shape, p, total, span, allow_no_inds=False):\n",
    "    # num_mask_spans = np.sum(np.random.rand(total) < p)\n",
    "    # num_mask_spans = int(p * total)\n",
    "    mask = torch.zeros(shape, requires_grad=False, dtype=torch.bool)\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        mask_seeds = list()\n",
    "        while not allow_no_inds and len(mask_seeds) == 0 and p > 0:\n",
    "            mask_seeds = np.nonzero(np.random.rand(total) < p)[0]\n",
    "\n",
    "        mask[i, _make_span_from_seeds(mask_seeds, span, total=total)] = True\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e79430c-e70e-4e42-8f35-3bbae19c2d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:50.579241200Z",
     "start_time": "2023-09-01T07:06:50.578245700Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        position = (position.T - position).T / max_len\n",
    "        self.register_buffer('rel_position', position)\n",
    "        \n",
    "        self.conv = torch.nn.Conv1d(max_len, d_model, 25, padding=25 // 2, groups=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        rel_pos = self.conv(self.rel_position[:x.size(1), :x.size(1)][None])[0].T\n",
    "        print(rel_pos.shape)\n",
    "        x = x + rel_pos\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ade54a7-ca7a-48c7-bb53-85a0f741d795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:50.873211700Z",
     "start_time": "2023-09-01T07:06:50.828243600Z"
    }
   },
   "outputs": [],
   "source": [
    "class EEGEmbedder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGEmbedder, self).__init__()\n",
    "        config = BertConfig(is_decoder=False, \n",
    "                    add_cross_attention=False,\n",
    "                    ff_layer='linear',\n",
    "                    hidden_size=512,\n",
    "                    num_attention_heads=8,\n",
    "                    num_hidden_layers=8,\n",
    "                    conv_kernel=1,\n",
    "                    conv_kernel_num=1)\n",
    "        self.model = BertEncoder(config)\n",
    "        \n",
    "        self.pos_e = PositionalEncoding(512, max_len=6000)\n",
    "        self.ch_embedder = torch.nn.Embedding(len(mitsar_chls), 512)\n",
    "        self.ch_norm = torch.nn.LayerNorm(512)\n",
    "        \n",
    "        self.input_norm = torch.nn.LayerNorm(2)\n",
    "        self.input_embedder = torch.nn.Sequential(\n",
    "            TransposeCustom(),\n",
    "            torch.nn.Conv1d(21, 32, 5, 2, padding=0),\n",
    "            torch.nn.Conv1d(32, 64, 5, 2, padding=0),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.GroupNorm(64 // 2, 64),\n",
    "            torch.nn.GELU(),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.Conv1d(64, 128, 3, 2, padding=0),\n",
    "            torch.nn.Conv1d(128, 196, 3, 2, padding=0),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.GroupNorm(196 // 2, 196),\n",
    "            torch.nn.GELU(),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.Conv1d(196, 256, 5, 1, padding=0),\n",
    "            torch.nn.Conv1d(256, 384, 5, 1, padding=0),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.GroupNorm(384 // 2, 384),\n",
    "            torch.nn.GELU(),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.Conv1d(384, 512, 5, 1, padding=0),\n",
    "            torch.nn.Conv1d(512, 512, 1, 1, padding=0),\n",
    "            torch.nn.GroupNorm(512 // 2, 512),\n",
    "            torch.nn.GELU(),\n",
    "            TransposeCustom(),\n",
    "        # torch.nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.output_embedder = torch.nn.Linear(512, 512)\n",
    "        self.transpose = TransposeCustom()\n",
    "        \n",
    "        self.mask_embedding = torch.nn.Parameter(torch.normal(0, 512**(-0.5), size=(512,)),\n",
    "                                                   requires_grad=True)\n",
    "        self.placeholder = torch.nn.Parameter(torch.normal(0, 512**(-0.5), size=(512,)),\n",
    "                                                   requires_grad=True)\n",
    "        \n",
    "    def forward(self, inputs, attention_mask, ch_vector):\n",
    "        embedding = self.input_embedder(inputs)\n",
    "        # create embedding for two channel indexes and sumup them to a single one\n",
    "        ch_embedding = self.ch_embedder(ch_vector).sum(1)\n",
    "        ch_embedding = ch_embedding[:, None]\n",
    "        embedding += ch_embedding\n",
    "        \n",
    "        # perform masking\n",
    "        embedding_unmasked = embedding.clone() # keep for loss calculation\n",
    "        mask = _make_mask((embedding.shape[0], embedding.shape[1]), 0.05, embedding.shape[1], 10)\n",
    "        embedding[mask] = self.mask_embedding\n",
    "        \n",
    "        # additional vector for classification tasks later\n",
    "        placeholder = torch.zeros((embedding.shape[0], 1, embedding.shape[2]), device=embedding.device)\n",
    "        placeholder += self.placeholder\n",
    "        embedding = torch.cat([placeholder, embedding], 1)\n",
    "        encoder_output = self.model(embedding, output_hidden_states=True,\n",
    "                               output_attentions=True)[0]\n",
    "        \n",
    "        encoder_output = self.output_embedder(encoder_output)\n",
    "        \n",
    "        return encoder_output[:, 1:], embedding_unmasked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77774c12-ce6f-47f6-87fe-9b158fdcfdee",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d6c08d-49a6-4789-a24f-929cb2bd294a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:51.459997900Z",
     "start_time": "2023-09-01T07:06:51.450844Z"
    }
   },
   "outputs": [],
   "source": [
    "def _generate_negatives(z):\n",
    "    \"\"\"Generate negative samples to compare each sequence location against\"\"\"\n",
    "    num_negatives = 20\n",
    "    batch_size, feat, full_len = z.shape\n",
    "    z_k = z.permute([0, 2, 1]).reshape(-1, feat)\n",
    "    with torch.no_grad():\n",
    "        # candidates = torch.arange(full_len).unsqueeze(-1).expand(-1, self.num_negatives).flatten()\n",
    "        negative_inds = torch.randint(0, full_len-1, size=(batch_size, full_len * num_negatives))\n",
    "        # From wav2vec 2.0 implementation, I don't understand\n",
    "        # negative_inds[negative_inds >= candidates] += 1\n",
    "\n",
    "        for i in range(1, batch_size):\n",
    "            negative_inds[i] += i * full_len\n",
    "\n",
    "    z_k = z_k[negative_inds.view(-1)].view(batch_size, full_len, num_negatives, feat)\n",
    "    return z_k, negative_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "534e0a28-d364-424b-95ea-ba9aef8969e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:51.837759800Z",
     "start_time": "2023-09-01T07:06:51.828727800Z"
    }
   },
   "outputs": [],
   "source": [
    "def _calculate_similarity( z, c, negatives):\n",
    "    c = c[..., :].permute([0, 2, 1]).unsqueeze(-2)\n",
    "    z = z.permute([0, 2, 1]).unsqueeze(-2)\n",
    "\n",
    "    # In case the contextualizer matches exactly, need to avoid divide by zero errors\n",
    "    negative_in_target = (c == negatives).all(-1)\n",
    "    targets = torch.cat([c, negatives], dim=-2)\n",
    "\n",
    "    logits = torch.nn.functional.cosine_similarity(z, targets, dim=-1) / 0.1\n",
    "    if negative_in_target.any():\n",
    "        logits[1:][negative_in_target] = float(\"-inf\")\n",
    "\n",
    "    return logits.view(-1, logits.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9679b5-eb5e-45e8-a042-88c24489b697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:52.059704Z",
     "start_time": "2023-09-01T07:06:52.018233400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39fbf98e-eee4-4232-b16d-33c35d176f72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:06:52.213042800Z",
     "start_time": "2023-09-01T07:06:52.173754400Z"
    }
   },
   "outputs": [],
   "source": [
    "def masking(ts):\n",
    "    start_shift = np.random.choice(range(10))\n",
    "    downsampling = 2\n",
    "    indices = np.random.choice(np.array(list(range(110)))[start_shift::10][::downsampling], 5, replace=False)\n",
    "    masked_idx = []\n",
    "    for i in indices:\n",
    "        masked_idx.extend(range(i, i+10))\n",
    "\n",
    "    masked_idx = np.array(masked_idx)\n",
    "    \n",
    "    # mask = np.ones((6000, 2))\n",
    "    # # desync some masked channels\n",
    "    # ts_masked = ts.copy()\n",
    "    # if np.random.choice([0, 1], p=[0.7, 0.3]):\n",
    "    #     ts_masked[masked_idx, np.random.choice([0, 1])] *= 0\n",
    "    # else:\n",
    "    #     ts_masked[masked_idx] *= 0\n",
    "        \n",
    "    return None, masked_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "003c6229-82b7-4dc3-9d01-5539f3ca1e74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:40.930369300Z",
     "start_time": "2023-09-01T07:11:40.888974200Z"
    }
   },
   "outputs": [],
   "source": [
    "class TEST(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(TEST, self).__init__()\n",
    "        self.main_path = path\n",
    "        self.paths = path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        # take 60s of recording with specified shift\n",
    "        key = False\n",
    "        while(key == False):\n",
    "            try:\n",
    "                sample = np.load(path, allow_pickle=True).item()\n",
    "                key = True\n",
    "            except Exception as e:\n",
    "                print(\"Path: {} is broken \".format(path), e)\n",
    "                path = np.random.choice(self.paths, 1)[0]\n",
    "                \n",
    "        signal = sample['value_pure']\n",
    "        real_len = signal.shape[0]\n",
    "        channels_ids = [i for i, val in enumerate(sample['channels']) if i != 3 and val in mitsar_chls]\n",
    "        print('sample[channels]', sample['channels'])\n",
    "        print('mitsar_chls', mitsar_chls)\n",
    "        print('channels_ids', channels_ids)\n",
    "        # choose 2 random channels\n",
    "        # channels_to_train = np.random.choice(channels_ids, 2, replace=False)\n",
    "        # channels_vector = torch.tensor((channels_to_train))\n",
    "        \n",
    "        # use all available\n",
    "        channels_to_train = channels_ids\n",
    "        channels_vector = torch.tensor((channels_to_train))\n",
    "        signal = signal[:, channels_to_train]\n",
    "\n",
    "        print('channels_to_train', channels_to_train)\n",
    "\n",
    "        # remove normalization for now with within channel z-norm\n",
    "        # sample_norm = (sample - tuh_filtered_stat_vals['min_vals_filtered'][channels_vector]) / (tuh_filtered_stat_vals['max_vals_filtered'][channels_vector] - tuh_filtered_stat_vals['min_vals_filtered'][channels_vector] + 1e-6)\n",
    "        sample_norm_mean = signal.mean()\n",
    "        sample_norm_std = np.std(signal)\n",
    "        \n",
    "        signal_norm = (signal - sample_norm_mean) / (sample_norm_std)\n",
    "        \n",
    "        if signal_norm.shape[0] < 6000:\n",
    "            signal_norm = np.pad(signal_norm, ((0, 6000 - signal_norm.shape[0]), (0, 0)))\n",
    "        \n",
    "        attention_mask = torch.ones(6000)\n",
    "        attention_mask[real_len:] = 0\n",
    "        return {'anchor': torch.from_numpy(signal_norm).float(), \n",
    "                # 'label': sample_label, \n",
    "                # 'anchor_masked': torch.from_numpy(sample_masked).float(), \n",
    "                # 'mask': torch.tensor(mask),\n",
    "                'channels': channels_vector,\n",
    "                'attention_mask': attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9402e789-65af-4b04-a351-620f227531b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:41.716918400Z",
     "start_time": "2023-09-01T07:11:41.653913400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 56 server\n",
    "# splitted_paths = ['/home/data/TUH_pretrain.filtered_1_40.v2.splited/{}'.format(i) for i in os.listdir('/home/data/TUH_pretrain.filtered_1_40.v2.splited/')]\n",
    "# 34 server\n",
    "splitted_paths = ['/media/hdd/data/TUH_pretrain.filtered_1_40.v2.splited/{}'.format(i) for i in os.listdir('/media/hdd/data/TUH_pretrain.filtered_1_40.v2.splited/')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9ad2e2d-c13c-4edd-9c7a-f214938b8ed5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:42.093197800Z",
     "start_time": "2023-09-01T07:11:41.993249100Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = np.load(np.random.choice(splitted_paths, 1)[0], allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30b779bf-9dad-4343-869d-72fcc79d5aa6",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:42.247872900Z",
     "start_time": "2023-09-01T07:11:42.203945300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array({'value': array([[  2.9273543 ,   6.77061426,  -2.80408903, ...,   5.15022475,\n         -0.17297896, -19.82115969],\n       [  6.93326314,  10.97923026,   0.67440623, ...,   9.75331321,\n          4.75569813, -23.13059152],\n       [ 12.67457194,  16.60059825,   5.67962452, ...,  16.59343865,\n         12.71718032,  -8.59118442],\n       ...,\n       [ 10.86360004,   8.23089587,   5.4728705 , ...,   0.85300305,\n          3.27642359, -11.6886919 ],\n       [ 13.62825731,   9.50276507,   6.48051907, ...,   0.15708415,\n          2.7950165 , -11.35647126],\n       [ 13.76536792,   8.60568661,   5.90039718, ...,   0.38953789,\n          0.76735608, -11.34078283]]), 'value_pure': array([[ -2.36025874,   3.68367735, -12.96940723, ..., -16.00250418,\n        -17.24266316, -19.82115969],\n       [ -6.86395286,  -0.67825386, -17.96700513, ..., -20.11534848,\n        -21.16590104, -23.13059152],\n       [  6.73416014,  12.99882206,  -4.87992788, ...,  -5.30467718,\n         -4.96709112,  -8.59118442],\n       ...,\n       [ 15.11578957,   5.98389448,  -3.97461362, ...,  -7.2985382 ,\n         -8.58567244, -11.6886919 ],\n       [ 17.72736547,   7.36715801,  -3.29843693, ...,  -8.23037432,\n         -9.46686247, -11.35647126],\n       [ 18.30965465,   6.77609712,  -3.33371199, ...,  -7.40938248,\n        -10.84284242, -11.34078283]]), 'date': 's016_2016_04_21', 'id': '142_00014239', 'ref_type': '01_tcp_ar', 'channels': ['FP1', 'FP2', 'FZ', 'FCZ', 'CZ', 'PZ', 'O1', 'O2', 'F3', 'F4', 'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', 'A1', 'A2', 'EKG1'], 'meta': 'HISTORY:\\u202878 y.o. female with a history of a right\\u2028frontal meningioma s/p gamma knife radiation in 2013,\\u2028parkinsonian features, HTN, DM, and arthritis, referred for EEG\\u2028for evaluation of new onset seizures: left gaze deviation and\\u2028left sided movements.\\u2028\\u2028MEDICATIONS:\\u2028Keppra\\u2028VPA\\u2028PHT\\u2028Lacosamide\\u2028\\u2028SEDATION: None documented\\u2028\\u2028TECHNIQUE:\\u2028A 21 channel electroencephalogram (EEG) was recorded using the\\u2028International 10-20 system with T1/T2 electrodes and utilized a\\u2028NicOne system. This was a technically satisfactory record and\\u2028included a single channel of EKG.\\u2028\\u2028**NOTE: This report describes the second half of a prolonged recording that began on 4/13/2016 15:22. \\xa0The findings from 4/13/2016 15:22 till 4/18/2016 05:59 are described in a separate report.\\u2028\\u2028DAY 6 (4/18/2016, 06:00 - 4/19/2016, 06:00)\\u2028\\u2028VPA 1000 mg q8h\\u2028PHT 100 mg q8h\\u2028Lacosamide 200 mg in AM, 200 mg in PM\\u2028Phenobarbital load 5 mg/kg q4-6h begun afternoon on this day\\u2028\\u2028BACKGROUND:\\u2028The waking background is continuous, reactive, poorly organized,\\u2028and asymmetric.\\u2028The left hemisphere consists of a disorganized mixture of mostly\\u2028theta with admixed alpha and delta. There is no posterior\\u2028dominant rhythm.\\u2028The right hemisphere consists of a disorganized mixture of theta\\u2028and delta with admixed alpha. There is no posterior dominant\\u2028rhythm.\\u2028\\u2028Drowsiness and sleep are characterized by further slowing of the\\u2028background. Sleep transients are not seen.\\u2028\\u2028There is moderate generalized slowing.\\u2028There is continuous right hemispheric polymorphic slowing.\\u2028\\u2028Study is disconnected on 4/18/2016 from 15:41-15:59 as patient is\\u2028transferred to neuro ICU. \\xa0Intubation occurs at 16:31 and is\\u2028associated with brief diffuse background attenuation. Following\\u2028intubation and initiation of phenobarbital load, the background\\u2028slows to a disorganized mixture of delta and theta, then mostly\\u2028delta with occasional admixed beta. At this point, there is no\\u2028definite reactivity or state change.\\u2028\\u2028PAROXSYMAL ACTIVITY (NON-EPILEPTIFORM): \\u2028None\\u2028\\u2028EPILEPTIFORM ACTIVITY: \\u20281. Continuous right temporal (T4 max) polyspikes +/- slow wave\\u2028(LPD+F / +/- LPD+FR), with broad field extending into right\\u2028hemisphere, occurring ever 0.5-1 Hz. \\u2028These persist after intubation and initiation of phenobarbital\\u2028load.\\u2028\\u2028EVENTS: Several seizures are captured:\\u20284/18/2016 at 06:10, 08:09, 10:40, 14:09, 16:25 (duration: 1 min\\u202830 sec, up to 3 min 12 sec)\\u2028CLINICAL: at times, these electrographic seizures are associated with\\u2028brief left head deviation, mouth opening, intermittent\\u2028guttural/groaning noises, and/or lip smacking beginning at least\\u2028~20-40 sec after electrographic onset\\u2028EEG: right temporal LPD+FR develop increasing underlying rhythmic\\u2028delta then break up into sharply contoured theta, then sharply\\u2028contoured delta with admixed spikes, with spread to the right\\u2028hemisphere.\\u2028\\u2028EKG: A single lead captured a heart rate paced at 80 bpm\\u2028\\u2028DAY 6 IMPRESSION & CLINICAL CORRELATION:\\u2028This is an abnormal long term video EEG due to:\\u20281. Moderate generalized slowing and background disorganization,\\u2028which worsens to severe generalized slowing after initiation of\\u2028phenobarbital load\\u20282. Continuous right hemispheric polymorphic slowing\\u20283. Right temporal LPD+FR\\u20284. Right temporal seizures, last noted 4/18/2016 at 16:25\\u2028\\u2028These findings are consistent localization related epilepsy of\\u2028right temporal origin.\\u2028There is additional evidence for moderate diffuse cerebral\\u2028dysfunction, worsening to severe generalized slowing, likely due\\u2028to medication effect.\\u2028There is additional evidence for right hemispheric dysfunction.\\u2028\\u2028\\u2028\\u2028\\u2028-----------------------------------------------------------------\\u2028\\u2028DAY 7 (4/19/2016, 06:00 - 4/20/2016, 06:00)\\u2028\\u2028VPA 1000 mg q8h\\u2028PHT 100 mg q8h\\u2028Lacosamide 200 mg in AM, 200 mg in PM\\u2028Phenobarbital load (20 mg/kg total) completed, level 27\\u2028Versed gtt started\\u2028\\u2028BACKGROUND: \\u2028Continuous with mostly delta with admixed beta\\u2028Around 4/19/2016 22:45, the background develops a burst\\u2028suppression pattern. Burst consist of 2-4 seconds of sharply\\u2028contoured theta. Suppression lasts 10-30 seconds.\\u2028\\u2028GENERALIZED SLOWING: severe\\u2028\\u2028FOCAL SLOWING: continuous right hemispheric slowing becomes less\\u2028noticeable as background slows\\u2028\\u2028PAROXSYMAL ACTIVITY (NON-EPILEPTIFORM): None\\u2028\\u2028SPORADIC EPILEPTIFORM ACTIVITY: \\u2028Continuous right temporal LPD+FR at 0.5 Hz. At times the\\u2028associated right temporal delta activity fluctuates without\\u2028definite evolution for periods < 10 seconds\\u2028After the background becomes burst suppression, R temporal\\u2028epileptiform discharges are no longer continuous but are abundant\\u2028(>1/10 sec).\\u2028\\u2028EVENTS: No clinical or electrographic events reported or detected\\u2028\\u2028EKG: A single EKG lead captured a heart rate in the 70s-80s.\\u2028\\u2028\\u2028DAY 7 IMPRESSION & CLINICAL CORRELATION\\u20281. Background slowing (severe) at least in part attributable to\\u2028medication effect (phenobarbital loading) \\u20282. Right temporal highly epileptogenic discharges persist, but no\\u2028definite electrographic seizures.\\u2028\\u2028\\u2028\\u2028\\u2028\\u2028\\u2028-----------------------------------------------------------------\\u2028\\u2028DAY 8 (4/20/2016, 06:00 - 4/21/2016, 06:00)\\u2028\\u2028BACKGROUND: burst suppression pattern. Bursts consist of 2-4 sec\\u2028of disorganized theta with admixed R temporal sharp waves.\\u2028Periods of suppression last 7-10 seconds.\\u2028\\u2028GENERALIZED SLOWING: severe\\u2028\\u2028FOCAL SLOWING: No change\\u2028\\u2028PAROXSYMAL ACTIVITY (NON-EPILEPTIFORM): None\\u2028\\u2028SPORADIC EPILEPTIFORM ACTIVITY: Frequent right temporal sharp\\u2028waves\\u2028\\u2028EVENTS: No clinical or electrographic events reported or detected\\u2028\\u2028EKG: A single EKG lead captured a heart rate in 70s-80s\\u2028\\u2028\\u2028DAY 8 IMPRESSION & CLINICAL CORRELATION\\u2028\\u2028Burst suppression pattern.\\u2028Frequent right temporal sharp waves.\\u2028\\u2028\\u2028\\u2028-----------------------------------------------------------------\\u2028\\u2028DAY 9 (4/21/2016, 06:00 - 4/22/2016, 06:00)\\u2028\\u2028Versed wean begun 4/21/2016 ~16:00\\u2028\\u2028BACKGROUND: As the versed wean is begun, the background gradually\\u2028becomes discontinuous, emerging from a burst suppression pattern.\\u2028Periods of discontinuity are briefer over the right temporal\\u2028region, which consists of fluctuating rhythmic delta activity and\\u2028abundant right temporal sharp waves.\\u2028\\u2028GENERALIZED SLOWING: No change\\u2028\\u2028FOCAL SLOWING: No change\\u2028\\u2028PAROXSYMAL ACTIVITY (NON-EPILEPTIFORM): None\\u2028\\u2028SPORADIC EPILEPTIFORM ACTIVITY: no change\\u2028\\u2028EVENTS: No clinical or electrographic events reported or detected\\u2028\\u2028EKG: A single EKG lead captured a heart rate in 70s-80s\\u2028\\u2028\\u2028DAY 9 IMPRESSION & CLINICAL CORRELATION\\u2028\\u2028No change compared with day 8.\\u2028\\u2028\\u2028\\u2028\\u2028-----------------------------------------------------------------\\u2028\\u2028DAY 10 (4/22/2016, 06:00 - 4/23/2016, 06:00)\\u2028\\u2028Versed wean completed ~4/22/2016 16:00\\u2028\\u2028BACKGROUND: Discontinuous as on Day 9. As the day progresses the\\u2028background becomes nearly continuous, with disorganized delta,\\u2028theta, and occasional alpha.\\u2028\\u2028GENERALIZED SLOWING: severe \\u2028\\u2028FOCAL SLOWING: No change\\u2028\\u2028PAROXSYMAL ACTIVITY (NON-EPILEPTIFORM): None\\u2028\\u2028SPORADIC EPILEPTIFORM ACTIVITY: very frequent right temporal\\u2028sharp waves persist\\u2028\\u2028EVENTS: No clinical or electrographic events reported or detected\\u2028\\u2028EKG: A single EKG lead captured a heart rate in the 80s\\u2028\\u2028\\u2028DAY 10 IMPRESSION & CLINICAL CORRELATION\\u2028\\u2028Discontinuous background. Right temporal sharp waves persist.\\u2028\\u2028\\u2028\\u2028-----------------------------------------------------------------\\u2028\\u2028DAY 11 (4/23/2016, 06:00 - 4/24/2016, 06:00)\\u2028\\u2028BACKGROUND: Gradually becomes continuous over the course of the\\u2028day, consisting of mostly theta and delta with admixed faster\\u2028frequencies. \\u2028\\u2028By 4/24/2016 02:00 the record is noted to be variable, with periods of alpha and beta alternating with delta and theta.\\u2028\\u2028Note that T4 electrode is affected by artifact from 4/23/2016\\u202806:34 until 4/23/2016 10:26, rendering evaluation of this region\\u2028suboptimal during this time.\\u2028\\u2028GENERALIZED SLOWING: Severe\\u2028\\u2028FOCAL SLOWING: Abundant quasi-rhythmic right temporal delta\\u2028slowing for runs <6 seconds\\u2028\\u2028PAROXSYMAL ACTIVITY (NON-EPILEPTIFORM): None\\u2028\\u2028SPORADIC EPILEPTIFORM ACTIVITY: frequent right temporal sharp\\u2028waves\\u2028\\u2028EVENTS: No clinical or electrographic events reported or detected\\u2028\\u2028EKG: A single EKG lead captured a heart rate in the 80s\\u2028\\u2028\\u2028DAY 11 IMPRESSION & CLINICAL CORRELATION\\u2028\\u2028Continuous background that becomes variable as day progresses.\\u2028Frequent right temporal sharp waves. \\u2028No electrographic seizures. Limited interpretation from 4/23/2016\\u202806:34-10:26.\\u2028\\u2028\\u2028\\u2028\\u2028\\u2028-----------------------------------------------------------------\\u2028\\u2028DAY 12 (4/24/2016, 06:00 - 4/24/2016, 10:34)\\u2028\\u2028BACKGROUND: variable, with periods of alpha and beta alternating with delta and theta\\u2028\\u2028GENERALIZED SLOWING: severe\\u2028\\u2028FOCAL SLOWING: Abundant quasi-rhythmic right temporal delta slowing for runs <6 seconds\\u2028\\u2028PAROXSYMAL ACTIVITY (NON-EPILEPTIFORM): None\\u2028\\u2028SPORADIC EPILEPTIFORM ACTIVITY: Occasional right temporal sharp waves\\u2028\\u2028EVENTS: No clinical or electrographic events reported or detected\\u2028\\u2028EKG: A single EKG lead captured a heart rate in the 80s\\u2028\\u2028\\u2028DAY 12 IMPRESSION & CLINICAL CORRELATION\\u2028\\u2028Continuous background with variability.\\u2028No clear reactivity/state change.\\u2028Severe generalized slowing.\\u2028Abundant very brief runs of right temporal quasi-rhythmic slowing.\\u2028Occasional right temporal sharp waves.\\u2028\\u2028\\u2028\\u2028-----------------------------------------------------------------\\u2028\\u2028FINAL IMPRESSION:\\u2028\\u2028This is an abnormal video EEG due to:\\u2028\\u20281. Electroclinical seizures of right temporal origin noted to persist until 4/18/2016 16:25\\u20282. Right temporal highly epileptogenic discharges that decrease in prevalence over course of study\\u20283. Right temporal slowing\\u20284. Background slowing, the degree of which correlates with medication effect\\u2028\\u2028\\u2028FINAL CLINICAL CORRELATION:\\u2028\\u2028These findings are diagnostic for localization related epilepsy of right temporal origin.\\u2028There is evidence for right temporal dysfunction, likely structural in etiology.\\u2028There is additional evidence for diffuse cerebral dysfunction. This worsens over the course of the study, correlating with administration of sedating medication, and improves by study end as sedating medication is down titrated.\\u2028\\u2028\\n'},\n      dtype=object)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b4279d9-4838-4748-a7c3-f6a51417dc59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:42.445956100Z",
     "start_time": "2023-09-01T07:11:42.404095600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "354bc4ee-ee88-402e-bea8-881e40f16314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:42.677293300Z",
     "start_time": "2023-09-01T07:11:42.594568900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "161710"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a413d97-8fc1-47a6-b137-3ac8247cf34b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:43.251099800Z",
     "start_time": "2023-09-01T07:11:43.246121700Z"
    }
   },
   "outputs": [],
   "source": [
    "mitsar_chls = ['Fp1', 'Fp2', 'FZ', 'FCz', 'Cz', 'Pz', 'O1', 'O2', 'F3', 'F4', \n",
    "               'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', 'A1', 'A2']\n",
    "mitsar_chls = [i.upper() for i in mitsar_chls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da834726-54e3-47a5-aee5-5559f2db6274",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:43.730116700Z",
     "start_time": "2023-09-01T07:11:43.719153200Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = TEST(splitted_paths)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "115f00e7-2dcb-4304-865f-57c352e7784f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:44.058896500Z",
     "start_time": "2023-09-01T07:11:44.054907500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cfd4bd0a-a2f1-4c2d-802b-04863b0cd9da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:11:44.406436300Z",
     "start_time": "2023-09-01T07:11:44.391559700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample[channels] ['FP1', 'FP2', 'FZ', 'FCZ', 'CZ', 'PZ', 'O1', 'O2', 'F3', 'F4', 'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', 'A1', 'A2', 'EKG1']\n",
      "mitsar_chls ['FP1', 'FP2', 'FZ', 'FCZ', 'CZ', 'PZ', 'O1', 'O2', 'F3', 'F4', 'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', 'A1', 'A2']\n",
      "channels_ids [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "channels_to_train [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'anchor': tensor([[ 0.3678,  0.0749,  0.0408,  ..., -0.0528,  0.1377,  0.0048],\n         [ 0.3655,  0.0216,  0.0055,  ...,  0.0235,  0.1583,  0.0343],\n         [ 0.3018, -0.0476,  0.0187,  ..., -0.0155,  0.0765, -0.0356],\n         ...,\n         [-0.3121, -0.1127, -0.2664,  ..., -0.3250, -0.4115, -0.3452],\n         [-0.3218, -0.1485, -0.2783,  ..., -0.2441, -0.4151, -0.2793],\n         [-0.2981, -0.1423, -0.2441,  ..., -0.1679, -0.3665, -0.2556]]),\n 'channels': tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n         19, 20, 21]),\n 'attention_mask': tensor([1., 1., 1.,  ..., 1., 1., 1.])}"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__getitem__(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a48eb7-0727-43ea-bedb-cd419ecd6532",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95ba1c89-cf1d-48d6-a6dd-4f8b0432ac95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:10:10.766822200Z",
     "start_time": "2023-09-01T07:10:10.240306900Z"
    }
   },
   "outputs": [],
   "source": [
    "model = EEGEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02c49688-e680-406f-9de5-2eae316eeb2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:10:10.772801400Z",
     "start_time": "2023-09-01T07:10:10.767817900Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50e2bc04-d8ab-4095-8081-c6e8d56fd6be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:10:12.768787500Z",
     "start_time": "2023-09-01T07:10:12.763873200Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class NoamLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, d_model=512):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.d_model = d_model\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        last_epoch = max(1, self.last_epoch)\n",
    "        factor = min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n",
    "        # scale = self.warmup_steps ** 0.5 * min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n",
    "        # return [base_lr * scale for base_lr in self.base_lrs]\n",
    "        return [base_lr * self.d_model ** (-0.5) * factor for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cd39442-574d-489d-9cae-2886fa7b2e7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:10:13.774197700Z",
     "start_time": "2023-09-01T07:10:13.762234400Z"
    }
   },
   "outputs": [],
   "source": [
    "cossim = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "def cosloss(anchor, real, negative):\n",
    "    a = torch.exp(cossim(anchor, real)) / 0.1\n",
    "    b = sum([torch.exp(cossim(anchor, negative[:, n])) / 0.1 for n in range(negative.shape[1])]) + 1e-6\n",
    "    return -torch.log(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e28e456c-e80d-4cf6-ad0b-4c6af1427908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:10:14.227938Z",
     "start_time": "2023-09-01T07:10:14.199028100Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def worker_init_fn(worker_id):\n",
    "    torch_seed = torch.initial_seed()\n",
    "    random.seed(torch_seed + worker_id)\n",
    "    np.random.seed((torch_seed + worker_id) % 2**30)\n",
    "\n",
    "train_dataset = TEST(splitted_paths[:-15000])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, drop_last=True, worker_init_fn = worker_init_fn)\n",
    "\n",
    "test_dataset = TEST(splitted_paths[-15000:])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, drop_last=True, worker_init_fn = worker_init_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a8e8d3c-6a8b-4af5-9d2d-98fb4eaa036c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:10:15.649487400Z",
     "start_time": "2023-09-01T07:10:15.644494300Z"
    }
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bc0dce6-c149-4043-bf79-d84a337177fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:10:16.146642700Z",
     "start_time": "2023-09-01T07:10:16.034471400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model.train()\n",
    "\n",
    "lr_d = 1\n",
    "acc_size = 8\n",
    "training_epochs1 = 100000 // len(train_loader)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr_d)\n",
    "\n",
    "model_test = torch.nn.DataParallel(model)\n",
    "model_test.to('cuda:0')\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "# scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=1.0, end_factor=0.1, total_iters=training_epochs1*len(train_loader))\n",
    "scheduler = NoamLR(optim, 100000, 512)\n",
    "\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25684f3b-1b46-41f3-ab64-342dd82ffb6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T07:10:17.115579800Z",
     "start_time": "2023-09-01T07:10:17.091660700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(2292, 43, 98556)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), training_epochs1, training_epochs1 * len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample[channels] ['FP1', 'FP2', 'FZ', 'FCZ', 'CZ', 'PZ', 'O1', 'O2', 'F3', 'F4', 'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', 'A1', 'A2', 'EKG1']\n",
      "channels_ids [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "channels_to_train [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "ch:  [3]\n"
     ]
    }
   ],
   "source": [
    "train_dataset[0]['anchor'].shape[:]\n",
    "aa = ['f1','c2','c3','d4']\n",
    "bb = ['c2','d4','d5']\n",
    "\n",
    "  # channels_ids = [i for i, val in enumerate(sample['channels']) if i != 3 and val in mitsar_chls]\n",
    "\n",
    "# qq = [i for i, val in enumerate(aa) if i != 1 and val in bb]\n",
    "qq = [i for i, val in enumerate(aa) if i != 1 and val in bb]\n",
    "print('ch: ', (qq))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T07:22:57.422918700Z",
     "start_time": "2023-09-01T07:22:57.377151500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([21])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['channels'].shape[:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-26T15:52:31.584490800Z",
     "start_time": "2023-08-26T15:52:31.571488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a8ee36c-93ab-408d-9171-951b2d01c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cpu()(batch['anchor'][None], batch['mask'][None], batch['channels'][None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24adbea7-974b-4d3a-bebe-99820033e7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 43)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch, training_epochs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2eceb-2ca0-4e7a-ba73-9443a284b696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b119cee-d759-4fd8-a8bf-130e8b8d17d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train\t2.707833170890808\n",
      "Loss/train\t2.7044030725955963\n",
      "Loss/train\t2.7018640637397766\n",
      "Loss/train\t2.702863395214081\n",
      "Loss/train\t2.7006741762161255\n",
      "Loss: 0.33758121728897095\t\n",
      "Loss/train\t2.6986430883407593\n",
      "Loss/train\t2.6988857984542847\n",
      "Loss/train\t2.6954116821289062\n",
      "Loss/train\t2.693866729736328\n",
      "Loss/train\t2.693163961172104\n",
      "Loss: 0.3367648124694824\t\n",
      "Loss/train\t2.6922953128814697\n",
      "Loss/train\t2.692641854286194\n",
      "Loss/train\t2.690729022026062\n",
      "Loss/train\t2.6889248192310333\n",
      "Loss/train\t2.687345862388611\n",
      "Loss: 0.33587104082107544\t\n",
      "Loss/train\t2.6858376562595367\n",
      "Loss/train\t2.685423821210861\n",
      "Loss/train\t2.6850564181804657\n",
      "Loss/train\t2.681473135948181\n",
      "Loss/train\t2.681659013032913\n",
      "Loss: 0.3349754810333252\t\n",
      "Loss/train\t2.6786417961120605\n",
      "Loss/train\t2.6785849034786224\n",
      "Loss/train\t2.676129937171936\n",
      "Loss/train\t2.67608904838562\n",
      "Loss/train\t2.6727095246315002\n",
      "Loss/train\t2.67220139503479\n",
      "Loss: 0.3341512382030487\t\n",
      "Loss/train\t2.6699333786964417\n",
      "Loss/train\t2.6697755455970764\n",
      "Loss/train\t2.6685484647750854\n",
      "Loss/train\t2.6661896109580994\n",
      "Loss/train\t2.666203945875168\n",
      "Loss: 0.33325356245040894\t\n",
      "Loss/train\t2.6645097136497498\n",
      "Loss/train\t2.66359481215477\n",
      "Loss/train\t2.6608680486679077\n",
      "Loss/train\t2.659908264875412\n",
      "Loss/train\t2.659434527158737\n",
      "Loss: 0.33233895897865295\t\n",
      "Loss/train\t2.659850090742111\n",
      "Loss/train\t2.6577040553092957\n",
      "Loss/train\t2.6535390615463257\n",
      "Loss/train\t2.6551953852176666\n",
      "Loss/train\t2.6545785665512085\n",
      "Loss: 0.33146288990974426\t\n",
      "Loss/train\t2.64870548248291\n",
      "Loss/train\t2.652191698551178\n",
      "Loss/train\t2.648951381444931\n",
      "Loss/train\t2.6468003690242767\n",
      "Loss/train\t2.6457305252552032\n",
      "Loss: 0.3305118978023529\t\n",
      "Loss/train\t2.64231076836586\n",
      "Loss/train\t2.642808258533478\n",
      "Loss/train\t2.6394462287425995\n",
      "Loss/train\t2.639981985092163\n",
      "Loss/train\t2.637433648109436\n",
      "Loss/train\t2.6383790969848633\n",
      "Loss: 0.32958725094795227\t\n",
      "Loss/train\t2.634442299604416\n",
      "Loss/train\t2.6348315477371216\n",
      "Loss/train\t2.6327807307243347\n",
      "Loss/train\t2.6321843564510345\n",
      "Loss/train\t2.6296715438365936\n",
      "Loss: 0.3286574184894562\t\n",
      "Loss/train\t2.6269637048244476\n",
      "Loss/train\t2.6268287301063538\n",
      "Loss/train\t2.626057893037796\n",
      "Loss/train\t2.627580612897873\n",
      "Loss/train\t2.622018665075302\n",
      "Loss: 0.3276914656162262\t\n",
      "Loss/train\t2.6212192475795746\n",
      "Loss/train\t2.61903378367424\n",
      "Loss/train\t2.6176157891750336\n",
      "Loss/train\t2.6154023706912994\n",
      "Loss/train\t2.6149121820926666\n",
      "Loss: 0.32674089074134827\t\n",
      "Loss/train\t2.613154500722885\n",
      "Loss/train\t2.611221343278885\n",
      "Loss/train\t2.6074445843696594\n",
      "Loss/train\t2.6087571382522583\n",
      "Loss/train\t2.6070752143859863\n",
      "Loss/train\t2.6060693860054016\n",
      "Loss: 0.3256882429122925\t\n",
      "Loss/train\t2.605550467967987\n",
      "Loss/train\t2.6030715107917786\n",
      "Loss/train\t2.599251300096512\n",
      "Loss/train\t2.597560852766037\n",
      "Loss/train\t2.5988745987415314\n",
      "Loss: 0.3246811628341675\t\n",
      "Loss/train\t2.595672219991684\n",
      "Loss/train\t2.59453022480011\n",
      "Loss/train\t2.592563897371292\n",
      "Loss/train\t2.590149313211441\n",
      "Loss/train\t2.5890387892723083\n",
      "Loss: 0.32363975048065186\t\n",
      "Loss/train\t2.5882093012332916\n",
      "Loss/train\t2.5862710773944855\n",
      "Loss/train\t2.5860415399074554\n",
      "Loss/train\t2.583000600337982\n",
      "Loss/train\t2.5797669291496277\n",
      "Loss: 0.32255852222442627\t\n",
      "Loss/train\t2.577721267938614\n",
      "Loss/train\t2.5794466137886047\n",
      "Loss/train\t2.5750776529312134\n",
      "Loss/train\t2.5747269690036774\n",
      "Loss/train\t2.5709992051124573\n",
      "Loss: 0.3214939832687378\t\n",
      "Loss/train\t2.571766823530197\n",
      "Loss/train\t2.5679008960723877\n",
      "Loss/train\t2.568170666694641\n",
      "Loss/train\t2.5670384764671326\n",
      "Loss/train\t2.5673218369483948\n",
      "Loss/train\t2.563549041748047\n",
      "Loss: 0.3203321099281311\t\n",
      "Loss/train\t2.5637864470481873\n",
      "Loss/train\t2.5561067163944244\n",
      "Loss/train\t2.555494099855423\n",
      "Loss/train\t2.5539298355579376\n",
      "Loss/train\t2.555213987827301\n",
      "Loss: 0.31914567947387695\t\n",
      "Loss/train\t2.550708144903183\n",
      "Loss/train\t2.552308291196823\n",
      "Loss/train\t2.549058198928833\n",
      "Loss/train\t2.544882148504257\n",
      "Loss/train\t2.546627789735794\n",
      "Loss: 0.31801843643188477\t\n",
      "Loss/train\t2.545820653438568\n",
      "Loss/train\t2.5406689047813416\n",
      "Loss/train\t2.541098564863205\n",
      "Loss/train\t2.5365327298641205\n",
      "Loss/train\t2.5350827276706696\n",
      "Loss: 0.31679418683052063\t\n",
      "Loss/train\t2.5308853685855865\n",
      "Loss/train\t2.5304401218891144\n",
      "Loss/train\t2.5267444849014282\n",
      "Loss/train\t2.526823788881302\n",
      "Loss/train\t2.523123472929001\n",
      "Loss/train\t2.52424156665802\n",
      "Loss: 0.31552985310554504\t\n",
      "Loss/train\t2.5206798315048218\n",
      "Loss/train\t2.521344870328903\n",
      "Loss/train\t2.5163914263248444\n",
      "Loss/train\t2.5141661167144775\n",
      "Loss/train\t2.51279479265213\n",
      "Loss: 0.31424930691719055\t\n",
      "Loss/train\t2.51125767827034\n",
      "Loss/train\t2.5111999213695526\n",
      "Loss/train\t2.5079477429389954\n",
      "Loss/train\t2.505753755569458\n",
      "Loss/train\t2.5026333928108215\n",
      "Loss: 0.3129315674304962\t\n",
      "Loss/train\t2.5017696022987366\n",
      "Loss/train\t2.5010052919387817\n",
      "Loss/train\t2.4958103597164154\n",
      "Loss/train\t2.4957588016986847\n",
      "Loss/train\t2.492697089910507\n",
      "Loss: 0.3115731477737427\t\n",
      "Loss/train\t2.491178661584854\n",
      "Loss/train\t2.4882166981697083\n",
      "Loss/train\t2.4851768612861633\n",
      "Loss/train\t2.484980344772339\n",
      "Loss/train\t2.479474186897278\n",
      "Loss: 0.3101842999458313\t\n",
      "Loss/train\t2.483993738889694\n",
      "Loss/train\t2.477712571620941\n",
      "Loss/train\t2.474809318780899\n",
      "Loss/train\t2.473330855369568\n",
      "Loss/train\t2.470761388540268\n",
      "Loss/train\t2.466443955898285\n",
      "Loss: 0.3087526261806488\t\n",
      "Loss/train\t2.4676129817962646\n",
      "Loss/train\t2.4626040160655975\n",
      "Loss/train\t2.4635086059570312\n",
      "Loss/train\t2.458750158548355\n",
      "Loss/train\t2.4591182470321655\n",
      "Loss: 0.3072536885738373\t\n",
      "Loss/train\t2.4571647346019745\n",
      "Loss/train\t2.4521292448043823\n",
      "Loss/train\t2.453030288219452\n",
      "Loss/train\t2.4470523595809937\n",
      "Loss: 0.30570870637893677\t\n",
      "Loss/train\t2.444253623485565\n",
      "Loss/train\t2.443483531475067\n",
      "Loss/train\t2.441421091556549\n",
      "Loss/train\t2.437271386384964\n",
      "Loss/train\t2.432971328496933\n",
      "Loss: 0.30422651767730713\t\n",
      "Loss/train\t2.43277245759964\n",
      "Loss/train\t2.4318210184574127\n",
      "Loss/train\t2.42548930644989\n",
      "Loss/train\t2.423831731081009\n",
      "Loss/train\t2.42037096619606\n",
      "Loss: 0.3025690019130707\t\n",
      "Loss/train\t2.419952392578125\n",
      "Loss/train\t2.416050910949707\n",
      "Loss/train\t2.4121221601963043\n",
      "Loss/train\t2.40858593583107\n",
      "Loss/train\t2.4086851477622986\n",
      "Loss: 0.30091845989227295\t\n",
      "Loss/train\t2.4065957069396973\n",
      "Loss/train\t2.403892010450363\n",
      "Loss/train\t2.3952824473381042\n",
      "Loss/train\t2.395994633436203\n",
      "Loss/train\t2.3949442207813263\n",
      "Loss: 0.2991071939468384\t\n",
      "Loss/train\t2.3920479118824005\n",
      "Loss/train\t2.3845600485801697\n",
      "Loss/train\t2.3830393254756927\n",
      "Loss/train\t2.3785060346126556\n",
      "Loss: 0.2972154915332794\t\n",
      "Loss/train\t2.377533733844757\n",
      "Loss/train\t2.3740662038326263\n",
      "Loss/train\t2.3696399331092834\n",
      "Loss/train\t2.367934912443161\n",
      "Loss/train\t2.362450212240219\n",
      "Loss: 0.29528722167015076\t\n",
      "Loss/train\t2.360601156949997\n",
      "Loss/train\t2.358331650495529\n",
      "Loss/train\t2.35499969124794\n",
      "Loss/train\t2.3498775362968445\n",
      "Loss/train\t2.34579735994339\n",
      "Loss: 0.29307013750076294\t\n",
      "Loss/train\t2.343650847673416\n",
      "Loss/train\t2.3377880454063416\n",
      "Loss/train\t2.335467576980591\n",
      "Loss/train\t2.3336543142795563\n",
      "Loss/train\t2.3265218138694763\n",
      "Loss: 0.29098063707351685\t\n",
      "Loss/train\t2.3276720345020294\n",
      "Loss/train\t2.320046991109848\n",
      "Loss/train\t2.3196070790290833\n",
      "Loss/train\t2.3138547539711\n",
      "Loss: 0.28892016410827637\t\n",
      "Loss/train\t2.310858517885208\n",
      "Loss/train\t2.3017964959144592\n",
      "Loss/train\t2.3023346066474915\n",
      "Loss/train\t2.2984957098960876\n",
      "Loss/train\t2.2949396073818207\n",
      "Loss: 0.28669291734695435\t\n",
      "Loss/train\t2.289785712957382\n",
      "Loss/train\t2.2899337708950043\n",
      "Loss/train\t2.287743091583252\n",
      "Loss/train\t2.282844841480255\n",
      "Loss/train\t2.2776551842689514\n",
      "Loss: 0.28453996777534485\t\n",
      "Loss/train\t2.2728090286254883\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(25, training_epochs1):\n",
    "    mean_loss = 0\n",
    "    acc_step = 0\n",
    "    for batch in train_loader:\n",
    "        ae, label = model_test(\n",
    "            batch['anchor'],#.to('cuda:0'), \n",
    "            None, \n",
    "            batch['channels'].long())\n",
    "        \n",
    "        logits = _calculate_similarity(torch.transpose(label, 1, 2), torch.transpose(ae, 1, 2), _generate_negatives(torch.transpose(label, 1, 2))[0])\n",
    "\n",
    "        fake_labels = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, fake_labels) + 0.001 * label.pow(2).mean()\n",
    "\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        mean_loss += loss.item()\n",
    "        acc_step += 1\n",
    "        steps += 1\n",
    "        # raise\n",
    "        if acc_step != 0 and acc_step % acc_size == 0:\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad()\n",
    "            if steps % 100 == 0:\n",
    "                print('Loss/train\\t{}'.format(mean_loss / acc_size))\n",
    "            writer.add_scalar('Loss/train', mean_loss / acc_size, steps)\n",
    "            mean_loss = 0\n",
    "        if steps != 0 and steps % 1000 == 0:\n",
    "            der = 0\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_loader:\n",
    "                        ae, label = model_test(\n",
    "                            batch['anchor'],#.to('cuda:0'), \n",
    "                            None, \n",
    "                            batch['channels'].long())\n",
    "                        logits = _calculate_similarity(torch.transpose(label, 1, 2), torch.transpose(ae, 1, 2), _generate_negatives(torch.transpose(label, 1, 2))[0])\n",
    "\n",
    "                        fake_labels = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)\n",
    "                        loss = torch.nn.CrossEntropyLoss()(logits, fake_labels) + 0.001 * label.pow(2).mean()\n",
    "\n",
    "                        loss = loss.mean() / acc_size\n",
    "                        der += loss\n",
    "                der /= len(test_loader)\n",
    "                writer.add_scalar('Loss/test', der, steps)\n",
    "\n",
    "                print('Loss: {}\\t'.format(der))\n",
    "            except:\n",
    "                raise\n",
    "            torch.save(model_test.module.state_dict(), 'models/step.pt'.format(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train\t12.755858182907104\n",
      "Loss/train\t12.643582820892334\n",
      "Loss/train\t12.454817771911621\n",
      "Loss/train\t12.185070753097534\n",
      "Loss/train\t11.838916063308716\n",
      "Loss: 1.477887749671936\t\n",
      "Loss/train\t11.409443974494934\n",
      "Loss/train\t10.90146255493164\n",
      "Loss/train\t10.320974111557007\n",
      "Loss/train\t9.67824900150299\n",
      "Loss/train\t8.992188572883606\n",
      "Loss: 1.1207029819488525\t\n",
      "Loss/train\t8.31241750717163\n",
      "Loss/train\t7.99466860294342\n",
      "Loss/train\t7.372100114822388\n",
      "Loss/train\t6.819593012332916\n",
      "Loss/train\t6.336482048034668\n",
      "Loss: 0.7635732293128967\t\n",
      "Loss/train\t5.912988305091858\n",
      "Loss/train\t5.541968882083893\n",
      "Loss/train\t5.225658714771271\n",
      "Loss/train\t4.952653110027313\n",
      "Loss/train\t4.719955623149872\n",
      "Loss: 0.576254665851593\t\n",
      "Loss/train\t4.516627132892609\n",
      "Loss/train\t4.346154153347015\n",
      "Loss/train\t4.199998915195465\n",
      "Loss/train\t4.136499226093292\n",
      "Loss/train\t4.017586529254913\n",
      "Loss/train\t3.9136194586753845\n",
      "Loss: 0.48888149857521057\t\n",
      "Loss/train\t3.8248919248580933\n",
      "Loss/train\t3.7474300265312195\n",
      "Loss/train\t3.6797587275505066\n",
      "Loss/train\t3.6180025935173035\n",
      "Loss/train\t3.5654504001140594\n",
      "Loss: 0.4454231560230255\t\n",
      "Loss/train\t3.517749160528183\n",
      "Loss/train\t3.477570593357086\n",
      "Loss/train\t3.4391528964042664\n",
      "Loss/train\t3.4062837064266205\n",
      "Loss/train\t3.3921675980091095\n",
      "Loss: 0.42217832803726196\t\n",
      "Loss/train\t3.365108996629715\n",
      "Loss/train\t3.339961439371109\n",
      "Loss/train\t3.319022983312607\n",
      "Loss/train\t3.299046367406845\n",
      "Loss/train\t3.2820617258548737\n",
      "Loss: 0.4091930687427521\t\n",
      "Loss/train\t3.2660966515541077\n",
      "Loss/train\t3.2521539628505707\n",
      "Loss/train\t3.238929182291031\n",
      "Loss/train\t3.2268972992897034\n",
      "Loss/train\t3.216195225715637\n",
      "Loss: 0.40142062306404114\t\n",
      "Loss/train\t3.2061351239681244\n",
      "Loss/train\t3.1923994719982147\n",
      "Loss/train\t3.1846416890621185\n",
      "Loss/train\t3.1765201687812805\n",
      "Loss/train\t3.1694579124450684\n",
      "Loss: 0.396173357963562\t\n",
      "Loss/train\t3.162583142518997\n",
      "Loss/train\t3.1557197868824005\n",
      "Loss/train\t3.1499793231487274\n",
      "Loss/train\t3.1444895267486572\n",
      "Loss/train\t3.138695240020752\n",
      "Loss: 0.3922988772392273\t\n",
      "Loss/train\t3.1322868168354034\n",
      "Loss/train\t3.1280214488506317\n",
      "Loss/train\t3.1260062754154205\n",
      "Loss/train\t3.1207349598407745\n",
      "Loss/train\t3.1165557503700256\n",
      "Loss: 0.38923749327659607\t\n",
      "Loss/train\t3.111479341983795\n",
      "Loss/train\t3.1068959534168243\n",
      "Loss/train\t3.1029442846775055\n",
      "Loss/train\t3.098528951406479\n",
      "Loss/train\t3.09549817442894\n",
      "Loss: 0.3866656422615051\t\n",
      "Loss/train\t3.092077612876892\n",
      "Loss/train\t3.0873886346817017\n",
      "Loss/train\t3.084429383277893\n",
      "Loss/train\t3.082130581140518\n",
      "Loss/train\t3.0791537761688232\n",
      "Loss/train\t3.075514942407608\n",
      "Loss: 0.38443100452423096\t\n",
      "Loss/train\t3.072321444749832\n",
      "Loss/train\t3.0686399936676025\n",
      "Loss/train\t3.0664722323417664\n",
      "Loss/train\t3.061935842037201\n",
      "Loss/train\t3.0598286986351013\n",
      "Loss: 0.3824310600757599\t\n",
      "Loss/train\t3.0558643341064453\n",
      "Loss/train\t3.0532078444957733\n",
      "Loss/train\t3.050230771303177\n",
      "Loss/train\t3.047433853149414\n",
      "Loss/train\t3.044699102640152\n",
      "Loss: 0.38062915205955505\t\n",
      "Loss/train\t3.043403059244156\n",
      "Loss/train\t3.0398872196674347\n",
      "Loss/train\t3.036885231733322\n",
      "Loss/train\t3.03528892993927\n",
      "Loss/train\t3.032084435224533\n",
      "Loss: 0.37899377942085266\t\n",
      "Loss/train\t3.031604826450348\n",
      "Loss/train\t3.0278061032295227\n",
      "Loss/train\t3.025331050157547\n",
      "Loss/train\t3.022030532360077\n",
      "Loss/train\t3.0207349956035614\n",
      "Loss: 0.37748050689697266\t\n",
      "Loss/train\t3.018094152212143\n",
      "Loss/train\t3.0176146924495697\n",
      "Loss/train\t3.0153778195381165\n",
      "Loss/train\t3.0125092566013336\n",
      "Loss/train\t3.009765535593033\n",
      "Loss/train\t3.0069188475608826\n",
      "Loss: 0.3760727643966675\t\n",
      "Loss/train\t3.006137728691101\n",
      "Loss/train\t3.0043415427207947\n",
      "Loss/train\t3.002405524253845\n",
      "Loss/train\t2.999369651079178\n",
      "Loss/train\t2.9971305429935455\n",
      "Loss: 0.37471261620521545\t\n",
      "Loss/train\t2.994408041238785\n",
      "Loss/train\t2.992760092020035\n",
      "Loss/train\t2.990832597017288\n",
      "Loss/train\t2.990477591753006\n",
      "Loss/train\t2.9877956807613373\n",
      "Loss: 0.3733205795288086\t\n",
      "Loss/train\t2.986118823289871\n",
      "Loss/train\t2.981015980243683\n",
      "Loss/train\t2.9802558720111847\n",
      "Loss/train\t2.9765120148658752\n",
      "Loss/train\t2.9765975773334503\n",
      "Loss: 0.3717987835407257\t\n",
      "Loss/train\t2.9724715650081635\n",
      "Loss/train\t2.9713196754455566\n",
      "Loss/train\t2.967473566532135\n",
      "Loss/train\t2.9643036127090454\n",
      "Loss/train\t2.9623642563819885\n",
      "Loss/train\t2.959965258836746\n",
      "Loss: 0.37001529335975647\t\n",
      "Loss/train\t2.956305652856827\n",
      "Loss/train\t2.9535030126571655\n",
      "Loss/train\t2.950118809938431\n",
      "Loss/train\t2.94686421751976\n",
      "Loss/train\t2.9447926580905914\n",
      "Loss: 0.36811453104019165\t\n",
      "Loss/train\t2.941439241170883\n",
      "Loss/train\t2.940099775791168\n",
      "Loss/train\t2.933786928653717\n",
      "Loss/train\t2.9334504902362823\n",
      "Loss/train\t2.9317177832126617\n",
      "Loss: 0.36634328961372375\t\n",
      "Loss/train\t2.928731918334961\n",
      "Loss/train\t2.925934761762619\n",
      "Loss/train\t2.924750953912735\n",
      "Loss/train\t2.922209084033966\n",
      "Loss/train\t2.918773740530014\n",
      "Loss: 0.36474496126174927\t\n",
      "Loss/train\t2.917858272790909\n",
      "Loss/train\t2.914899706840515\n",
      "Loss/train\t2.912205785512924\n",
      "Loss/train\t2.9107946157455444\n",
      "Loss/train\t2.9089353680610657\n",
      "Loss: 0.3633054494857788\t\n",
      "Loss/train\t2.9032803773880005\n",
      "Loss/train\t2.9044482707977295\n",
      "Loss/train\t2.901307851076126\n",
      "Loss/train\t2.9011556804180145\n",
      "Loss/train\t2.8967393338680267\n",
      "Loss/train\t2.8948283791542053\n",
      "Loss: 0.3619936406612396\t\n",
      "Loss/train\t2.893488496541977\n",
      "Loss/train\t2.8945826292037964\n",
      "Loss/train\t2.890445441007614\n",
      "Loss/train\t2.889322578907013\n",
      "Loss/train\t2.8862860202789307\n",
      "Loss: 0.36085671186447144\t\n",
      "Loss/train\t2.8846550583839417\n",
      "Loss/train\t2.8824138939380646\n",
      "Loss/train\t2.8805632889270782\n",
      "Loss/train\t2.877865046262741\n",
      "Loss: 0.3597930073738098\t\n",
      "Loss/train\t2.876994550228119\n",
      "Loss/train\t2.8748088777065277\n",
      "Loss/train\t2.8739466071128845\n",
      "Loss/train\t2.8730830550193787\n",
      "Loss/train\t2.8697808980941772\n",
      "Loss: 0.35880935192108154\t\n",
      "Loss/train\t2.8698234260082245\n",
      "Loss/train\t2.8656990826129913\n",
      "Loss/train\t2.866015374660492\n",
      "Loss/train\t2.8658622801303864\n",
      "Loss/train\t2.8644455671310425\n",
      "Loss: 0.35783228278160095\t\n",
      "Loss/train\t2.863614559173584\n",
      "Loss/train\t2.8591532707214355\n",
      "Loss/train\t2.856845945119858\n",
      "Loss/train\t2.855754554271698\n",
      "Loss/train\t2.854324847459793\n",
      "Loss: 0.35694777965545654\t\n",
      "Loss/train\t2.8533268868923187\n",
      "Loss/train\t2.854556620121002\n",
      "Loss/train\t2.8517045974731445\n",
      "Loss/train\t2.8503537476062775\n",
      "Loss/train\t2.848848044872284\n",
      "Loss: 0.35607069730758667\t\n",
      "Loss/train\t2.8469757437705994\n",
      "Loss/train\t2.845758020877838\n",
      "Loss/train\t2.8425829708576202\n",
      "Loss/train\t2.8419530391693115\n",
      "Loss: 0.355116069316864\t\n",
      "Loss/train\t2.8403874933719635\n",
      "Loss/train\t2.838493049144745\n",
      "Loss/train\t2.8370776772499084\n",
      "Loss/train\t2.837918758392334\n",
      "Loss/train\t2.832762748003006\n",
      "Loss: 0.35413530468940735\t\n",
      "Loss/train\t2.8325993716716766\n",
      "Loss/train\t2.8309223651885986\n",
      "Loss/train\t2.8307434916496277\n",
      "Loss/train\t2.8266052305698395\n",
      "Loss/train\t2.827120989561081\n",
      "Loss: 0.35319024324417114\t\n",
      "Loss/train\t2.823172479867935\n",
      "Loss/train\t2.824467718601227\n",
      "Loss/train\t2.820710629224777\n",
      "Loss/train\t2.819431245326996\n",
      "Loss/train\t2.8206507861614227\n",
      "Loss: 0.3523656725883484\t\n",
      "Loss/train\t2.819383680820465\n",
      "Loss/train\t2.817619800567627\n",
      "Loss/train\t2.814256966114044\n",
      "Loss/train\t2.814727395772934\n",
      "Loss: 0.3515779972076416\t\n",
      "Loss/train\t2.8120080530643463\n",
      "Loss/train\t2.810046136379242\n",
      "Loss/train\t2.810254007577896\n",
      "Loss/train\t2.8101383447647095\n",
      "Loss/train\t2.8087089359760284\n",
      "Loss: 0.3508453965187073\t\n",
      "Loss/train\t2.8048042356967926\n",
      "Loss/train\t2.8043673634529114\n",
      "Loss/train\t2.8037441074848175\n",
      "Loss/train\t2.8035619258880615\n",
      "Loss/train\t2.8002596497535706\n",
      "Loss: 0.35011374950408936\t\n",
      "Loss/train\t2.8011174499988556\n",
      "Loss/train\t2.8016227781772614\n",
      "Loss/train\t2.7980417907238007\n",
      "Loss/train\t2.79611673951149\n",
      "Loss/train\t2.794576644897461\n",
      "Loss: 0.34943118691444397\t\n",
      "Loss/train\t2.7949137091636658\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(training_epochs1):\n",
    "    mean_loss = 0\n",
    "    acc_step = 0\n",
    "    for batch in train_loader:\n",
    "        ae, label = model_test(\n",
    "            batch['anchor'],#.to('cuda:0'),\n",
    "            None,\n",
    "            batch['channels'].long())\n",
    "\n",
    "        logits = _calculate_similarity(torch.transpose(label, 1, 2), torch.transpose(ae, 1, 2), _generate_negatives(torch.transpose(label, 1, 2))[0])\n",
    "\n",
    "        fake_labels = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, fake_labels) + 0.001 * label.pow(2).mean()\n",
    "\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        mean_loss += loss.item()\n",
    "        acc_step += 1\n",
    "        steps += 1\n",
    "        # raise\n",
    "        if acc_step != 0 and acc_step % acc_size == 0:\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad()\n",
    "            if steps % 100 == 0:\n",
    "                print('Loss/train\\t{}'.format(mean_loss / acc_size))\n",
    "            writer.add_scalar('Loss/train', mean_loss / acc_size, steps)\n",
    "            mean_loss = 0\n",
    "        if steps != 0 and steps % 1000 == 0:\n",
    "            der = 0\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_loader:\n",
    "                        ae, label = model_test(\n",
    "                            batch['anchor'],#.to('cuda:0'),\n",
    "                            None,\n",
    "                            batch['channels'].long())\n",
    "                        logits = _calculate_similarity(torch.transpose(label, 1, 2), torch.transpose(ae, 1, 2), _generate_negatives(torch.transpose(label, 1, 2))[0])\n",
    "\n",
    "                        fake_labels = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)\n",
    "                        loss = torch.nn.CrossEntropyLoss()(logits, fake_labels) + 0.001 * label.pow(2).mean()\n",
    "\n",
    "                        loss = loss.mean() / acc_size\n",
    "                        der += loss\n",
    "                der /= len(test_loader)\n",
    "                writer.add_scalar('Loss/test', der, steps)\n",
    "\n",
    "                print('Loss: {}\\t'.format(der))\n",
    "            except:\n",
    "                raise\n",
    "            torch.save(model_test.module.state_dict(), 'models/step.pt'.format(steps))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ee4f4-4764-44d4-b27f-eb1da2804fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56e39d-e163-4e2f-a0d4-1825f5921796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728f769-bff6-4ecf-b750-a3e183a65366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0300b-9b95-4741-961f-f3b471ad7005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
