{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03477f44-46c2-4a22-aaee-a67878c1816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 10 06:27:39 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.78.01    Driver Version: 525.78.01    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:19:00.0 Off |                  N/A |\n",
      "| 30%   31C    P5    42W / 350W |    774MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:65:00.0 Off |                  N/A |\n",
      "| 30%   33C    P5    97W / 350W |      8MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1704      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    900432      C   /usr/bin/python3                  766MiB |\n",
      "|    1   N/A  N/A      1704      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e513c8-c521-4ac4-b93c-bea7dbb4eac9",
   "metadata": {},
   "source": [
    "##### %config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de61d6ab-33e9-40ef-b448-6686a5a4e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# os.environ['http_proxy'] = \"http://127.0.0.1:3128\"\n",
    "# os.environ['https_proxy'] = \"http://127.0.0.1:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab062da-1f4d-4f54-965f-49e519bb897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a029d2ef-c060-410a-9c98-aa48d86c7926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e1e8bc-99d9-4968-9956-fe13fc4433e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c443a-6995-43ca-ac1d-43cfe5ec376a",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ef112d9-8b15-407a-b735-cc584072cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposeCustom(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransposeCustom, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.transpose(x, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "411dce92-23fd-4258-ab4c-73b9f68f8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = BertConfig(is_decoder=True, \n",
    "#                     add_cross_attention=True,\n",
    "#                     ff_layer='conv',\n",
    "#                     conv_kernel=1,\n",
    "#                     conv_kernel_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711ae813-492d-4fe1-89ee-8315cb43617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_span_from_seeds(seeds, span, total=None):\n",
    "    inds = list()\n",
    "    for seed in seeds:\n",
    "        for i in range(seed, seed + span):\n",
    "            if total is not None and i >= total:\n",
    "                break\n",
    "            elif i not in inds:\n",
    "                inds.append(int(i))\n",
    "    return np.array(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d78b6b1-0a91-4706-92ca-f52a791ea753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_mask(shape, p, total, span, allow_no_inds=False):\n",
    "    # num_mask_spans = np.sum(np.random.rand(total) < p)\n",
    "    # num_mask_spans = int(p * total)\n",
    "    mask = torch.zeros(shape, requires_grad=False, dtype=torch.bool)\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        mask_seeds = list()\n",
    "        while not allow_no_inds and len(mask_seeds) == 0 and p > 0:\n",
    "            mask_seeds = np.nonzero(np.random.rand(total) < p)[0]\n",
    "\n",
    "        mask[i, _make_span_from_seeds(mask_seeds, span, total=total)] = True\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e79430c-e70e-4e42-8f35-3bbae19c2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        position = (position.T - position).T / max_len\n",
    "        self.register_buffer('rel_position', position)\n",
    "        \n",
    "        self.conv = torch.nn.Conv1d(max_len, d_model, 25, padding=25 // 2, groups=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        rel_pos = self.conv(self.rel_position[:x.size(1), :x.size(1)][None])[0].T\n",
    "        print(rel_pos.shape)\n",
    "        x = x + rel_pos\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ade54a7-ca7a-48c7-bb53-85a0f741d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGEmbedder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGEmbedder, self).__init__()\n",
    "        config = BertConfig(is_decoder=False, \n",
    "                    add_cross_attention=False,\n",
    "                    ff_layer='linear',\n",
    "                    hidden_size=512,\n",
    "                    num_attention_heads=8,\n",
    "                    num_hidden_layers=8,\n",
    "                    conv_kernel=1,\n",
    "                    conv_kernel_num=1)\n",
    "        self.model = BertEncoder(config)\n",
    "        \n",
    "        self.pos_e = PositionalEncoding(512, max_len=6000)\n",
    "        self.ch_embedder = torch.nn.Embedding(len(mitsar_chls), 512)\n",
    "        self.ch_norm = torch.nn.LayerNorm(512)\n",
    "        \n",
    "        self.input_norm = torch.nn.LayerNorm(2)\n",
    "        self.input_embedder = torch.nn.Sequential(\n",
    "            TransposeCustom(),\n",
    "            torch.nn.Conv1d(21, 32, 5, 2, padding=0),\n",
    "            torch.nn.Conv1d(32, 64, 5, 2, padding=0),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.GroupNorm(64 // 2, 64),\n",
    "            torch.nn.GELU(),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.Conv1d(64, 128, 3, 2, padding=0),\n",
    "            torch.nn.Conv1d(128, 196, 3, 2, padding=0),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.GroupNorm(196 // 2, 196),\n",
    "            torch.nn.GELU(),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.Conv1d(196, 256, 5, 1, padding=0),\n",
    "            torch.nn.Conv1d(256, 384, 5, 1, padding=0),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.GroupNorm(384 // 2, 384),\n",
    "            torch.nn.GELU(),\n",
    "            # TransposeCustom(),\n",
    "            torch.nn.Conv1d(384, 512, 5, 1, padding=0),\n",
    "            torch.nn.Conv1d(512, 512, 1, 1, padding=0),\n",
    "            torch.nn.GroupNorm(512 // 2, 512),\n",
    "            torch.nn.GELU(),\n",
    "            TransposeCustom(),\n",
    "        # torch.nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.output_embedder = torch.nn.Linear(512, 512)\n",
    "        self.transpose = TransposeCustom()\n",
    "        \n",
    "        self.mask_embedding = torch.nn.Parameter(torch.normal(0, 512**(-0.5), size=(512,)),\n",
    "                                                   requires_grad=True)\n",
    "        self.placeholder = torch.nn.Parameter(torch.normal(0, 512**(-0.5), size=(512,)),\n",
    "                                                   requires_grad=True)\n",
    "        \n",
    "    def forward(self, inputs, attention_mask, ch_vector):\n",
    "        embedding = self.input_embedder(inputs)\n",
    "        # create embedding for two channel indexes and sumup them to a single one\n",
    "        ch_embedding = self.ch_embedder(ch_vector).sum(1)\n",
    "        ch_embedding = ch_embedding[:, None]\n",
    "        embedding += ch_embedding\n",
    "        \n",
    "        # perform masking\n",
    "        embedding_unmasked = embedding.clone() # keep for loss calculation\n",
    "        mask = _make_mask((embedding.shape[0], embedding.shape[1]), 0.05, embedding.shape[1], 10)\n",
    "        embedding[mask] = self.mask_embedding\n",
    "        \n",
    "        # additional vector for classification tasks later\n",
    "        placeholder = torch.zeros((embedding.shape[0], 1, embedding.shape[2]), device=embedding.device)\n",
    "        placeholder += self.placeholder\n",
    "        embedding = torch.cat([placeholder, embedding], 1)\n",
    "        encoder_output = self.model(embedding, output_hidden_states=True,\n",
    "                               output_attentions=True)[0]\n",
    "        \n",
    "        encoder_output = self.output_embedder(encoder_output)\n",
    "        \n",
    "        return encoder_output[:, 1:], embedding_unmasked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77774c12-ce6f-47f6-87fe-9b158fdcfdee",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9d6c08d-49a6-4789-a24f-929cb2bd294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_negatives(z):\n",
    "    \"\"\"Generate negative samples to compare each sequence location against\"\"\"\n",
    "    num_negatives = 20\n",
    "    batch_size, feat, full_len = z.shape\n",
    "    z_k = z.permute([0, 2, 1]).reshape(-1, feat)\n",
    "    with torch.no_grad():\n",
    "        # candidates = torch.arange(full_len).unsqueeze(-1).expand(-1, self.num_negatives).flatten()\n",
    "        negative_inds = torch.randint(0, full_len-1, size=(batch_size, full_len * num_negatives))\n",
    "        # From wav2vec 2.0 implementation, I don't understand\n",
    "        # negative_inds[negative_inds >= candidates] += 1\n",
    "\n",
    "        for i in range(1, batch_size):\n",
    "            negative_inds[i] += i * full_len\n",
    "\n",
    "    z_k = z_k[negative_inds.view(-1)].view(batch_size, full_len, num_negatives, feat)\n",
    "    return z_k, negative_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "534e0a28-d364-424b-95ea-ba9aef8969e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_similarity( z, c, negatives):\n",
    "    c = c[..., :].permute([0, 2, 1]).unsqueeze(-2)\n",
    "    z = z.permute([0, 2, 1]).unsqueeze(-2)\n",
    "\n",
    "    # In case the contextualizer matches exactly, need to avoid divide by zero errors\n",
    "    negative_in_target = (c == negatives).all(-1)\n",
    "    targets = torch.cat([c, negatives], dim=-2)\n",
    "\n",
    "    logits = torch.nn.functional.cosine_similarity(z, targets, dim=-1) / 0.1\n",
    "    if negative_in_target.any():\n",
    "        logits[1:][negative_in_target] = float(\"-inf\")\n",
    "\n",
    "    return logits.view(-1, logits.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9679b5-eb5e-45e8-a042-88c24489b697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39fbf98e-eee4-4232-b16d-33c35d176f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking(ts):\n",
    "    start_shift = np.random.choice(range(10))\n",
    "    downsampling = 2\n",
    "    indices = np.random.choice(np.array(list(range(110)))[start_shift::10][::downsampling], 5, replace=False)\n",
    "    masked_idx = []\n",
    "    for i in indices:\n",
    "        masked_idx.extend(range(i, i+10))\n",
    "\n",
    "    masked_idx = np.array(masked_idx)\n",
    "    \n",
    "    # mask = np.ones((6000, 2))\n",
    "    # # desync some masked channels\n",
    "    # ts_masked = ts.copy()\n",
    "    # if np.random.choice([0, 1], p=[0.7, 0.3]):\n",
    "    #     ts_masked[masked_idx, np.random.choice([0, 1])] *= 0\n",
    "    # else:\n",
    "    #     ts_masked[masked_idx] *= 0\n",
    "        \n",
    "    return None, masked_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "003c6229-82b7-4dc3-9d01-5539f3ca1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEST(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        super(TEST, self).__init__()\n",
    "        self.main_path = path\n",
    "        self.paths = path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        # take 60s of recording with specified shift\n",
    "        key = False\n",
    "        while(key == False):\n",
    "            try:\n",
    "                sample = np.load(path, allow_pickle=True).item()\n",
    "                key = True\n",
    "            except Exception as e:\n",
    "                print(\"Path: {} is broken \".format(path), e)\n",
    "                path = np.random.choice(self.paths, 1)[0]\n",
    "                \n",
    "        signal = sample['value_pure']\n",
    "        real_len = signal.shape[0]\n",
    "        channels_ids = [i for i, val in enumerate(sample['channels']) if i != 3 and val in mitsar_chls]\n",
    "        \n",
    "        # choose 2 random channels\n",
    "        # channels_to_train = np.random.choice(channels_ids, 2, replace=False)\n",
    "        # channels_vector = torch.tensor((channels_to_train))\n",
    "        \n",
    "        # use all available\n",
    "        channels_to_train = channels_ids\n",
    "        channels_vector = torch.tensor((channels_to_train))\n",
    "        signal = signal[:, channels_to_train]\n",
    "        \n",
    "        # remove normalization for now with within channel z-norm\n",
    "        # sample_norm = (sample - tuh_filtered_stat_vals['min_vals_filtered'][channels_vector]) / (tuh_filtered_stat_vals['max_vals_filtered'][channels_vector] - tuh_filtered_stat_vals['min_vals_filtered'][channels_vector] + 1e-6)\n",
    "        sample_norm_mean = signal.mean()\n",
    "        sample_norm_std = np.std(signal)\n",
    "        \n",
    "        signal_norm = (signal - sample_norm_mean) / (sample_norm_std)\n",
    "        \n",
    "        if signal_norm.shape[0] < 6000:\n",
    "            signal_norm = np.pad(signal_norm, ((0, 6000 - signal_norm.shape[0]), (0, 0)))\n",
    "        \n",
    "        attention_mask = torch.ones(6000)\n",
    "        attention_mask[real_len:] = 0\n",
    "        return {'anchor': torch.from_numpy(signal_norm).float(), \n",
    "                # 'label': sample_label, \n",
    "                # 'anchor_masked': torch.from_numpy(sample_masked).float(), \n",
    "                # 'mask': torch.tensor(mask),\n",
    "                'channels': channels_vector,\n",
    "                'attention_mask': attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9402e789-65af-4b04-a351-620f227531b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_paths = ['/home/data/TUH_pretrain.filtered_1_40.v2.splited/{}'.format(i) for i in os.listdir('/home/data/TUH_pretrain.filtered_1_40.v2.splited/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ad2e2d-c13c-4edd-9c7a-f214938b8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.load(np.random.choice(splitted_paths, 1)[0], allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b779bf-9dad-4343-869d-72fcc79d5aa6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'value': array([[-4.42499326, -2.7009991 , -0.20808444, ..., -0.060136  ,\n",
       "        -0.74355037,  4.53297493],\n",
       "       [-4.64607219, -2.42082983, -1.29204337, ..., -1.49210348,\n",
       "        -0.44337056,  4.21196705],\n",
       "       [-5.86530095, -2.9173413 , -3.36674629, ..., -6.52046868,\n",
       "        -0.08575566,  3.72641073],\n",
       "       ...,\n",
       "       [-8.63263434, -5.79216501, -5.84941233, ...,  1.64683733,\n",
       "         1.70556313, 12.95027482],\n",
       "       [-8.31597699, -6.26158198, -5.08648961, ...,  2.5075473 ,\n",
       "        -0.11007407, 15.10993544],\n",
       "       [-3.990207  , -4.59186809, -1.28858423, ...,  2.99434554,\n",
       "         1.49910591, 13.20202265]]), 'value_pure': array([[ 3.83658262,  2.99540549,  5.57197208, ..., -0.0204242 ,\n",
       "         4.00498252,  4.53297493],\n",
       "       [ 4.07004778,  3.74585698,  4.573094  , ..., -0.18060649,\n",
       "         4.54117105,  4.21196705],\n",
       "       [ 2.28651574,  2.79767423,  2.34105158, ..., -6.42320109,\n",
       "         4.84064746,  3.72641073],\n",
       "       ...,\n",
       "       [ 1.83753667,  1.80770625,  0.28032674, ...,  7.242829  ,\n",
       "         8.31978099, 12.95027482],\n",
       "       [ 3.35314935,  1.57497306,  1.60874081, ...,  9.36608815,\n",
       "         6.41038129, 15.10993544],\n",
       "       [ 6.79482723,  3.20641228,  4.99467717, ...,  8.59088139,\n",
       "         8.34536809, 13.20202265]]), 'date': 's002_2016_06_23', 'id': '148_00014812', 'ref_type': '01_tcp_ar', 'channels': ['FP1', 'FP2', 'FZ', 'FCZ', 'CZ', 'PZ', 'O1', 'O2', 'F3', 'F4', 'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', 'A1', 'A2', 'EKG1'], 'meta': 'START DATE OF STUDY: 06/23/16 at 16:32\\u2028END DATE OF STUDY: 06/24/16 at 15:25\\u2028\\u2028RECORDING ENVIRONMENT: \\u2028EEG TYPE: long term monitoring\\u2028\\u2028\\u2028HISTORY:\\u2028a 52 y.o. male with a history of bilateral pulmonary emboli, pneumonia, on ECMO, who is referred for EEG for evaluation of paroxysmal events (proximal LUE twitching) and assessment of mental status change (stuporous).\\u2028\\u2028MEDICATIONS: versed, fentanyl, insulin, albumin, others\\u2028\\u2028SEDATION: as above\\u2028\\u2028TECHNIQUE:\\u2028A 21 channel electroencephalogram (EEG) was recorded using the International 10-20 system with T1/T2 electrodes and utilized a NicOne system. This was a technically satisfactory record and included a single channel of EKG.\\u2028\\u2028\\u2028DAY 1 (6/23/2016, 16:32 - 6/24/2016, 15:25)\\u2028\\u2028BACKGROUND:\\u2028The waking background is continuous and symmetric but poorly organized, without a posterior dominant rhythm or sleep transients. The background is variable, with periods of diffuse alpha alternating with diffuse low amplitude delta with admixed theta.\\u2028\\u2028Occasionally subtle attenuation of faster frequencies is noted over the right hemisphere.\\u2028There is moderate to severe generalized slowing.\\u2028\\u2028PAROXSYMAL ACTIVITY (NON-EPILEPTIFORM): \\u2028None\\u2028\\u2028EPILEPTIFORM ACTIVITY: \\u2028None\\u2028\\u2028EVENTS: \\u2028Two push button events on 6/23/2016 at 23:53, 23:58\\u2028CLINICAL: Nonrhythmic fine tremor of left shoulder that spread to involve trunk\\u2028EEG: No ictal EEG correlate, other than muscle artifact, is present\\u2028EKG: No change in baseline heart rate\\u2028\\u2028One push button event on 6/24/2016 at 08:08\\u2028CLINICAL: Patient obscured by camera angle and blankets, but appears to have same tremulous movements of body\\u2028EEG: No ictal EEG correlate, other than muscle artifact, is present\\u2028EKG: No change in baseline heart rate\\u2028\\u2028\\u2028EKG: A single lead captured a heart rate in the 80s\\u2028\\u2028IMPRESSION:\\u2028This is an abnormal long term video EEG due to:\\u20281. Moderate to severe generalized slowing with poor organization\\u20282. Subtle attenuation of faster frequencies seen intermittently over the right hemisphere\\u20283. Three push button events for tremulous movements as detailed above, without EEG correlate\\u2028\\u2028\\u2028CLINICAL CORRELATION:\\u2028These findings are etiologically nonspecific indicators of moderate to severe diffuse cerebral dysfunction.\\u2028\\u2028Attenuation of faster frequencies over the right hemisphere can be consistent either with cortical dysfunction over this region, or alternatively can be seen with any etiology that increases the distance between the cortex and recording electrodes, such as subdural hematoma, epidural hematoma, or scalp hematoma - clinical correlation is advised.\\u2028\\u2028Three push button events for tremulous movements as detailed above had no EEG cerebral correlate and are unlikely to represent seizure activity.\\u2028\\u2028\\u2028Reading attending: \\n'},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4279d9-4838-4748-a7c3-f6a51417dc59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "354bc4ee-ee88-402e-bea8-881e40f16314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161710"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a413d97-8fc1-47a6-b137-3ac8247cf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitsar_chls = ['Fp1', 'Fp2', 'FZ', 'FCz', 'Cz', 'Pz', 'O1', 'O2', 'F3', 'F4', \n",
    "               'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', 'A1', 'A2']\n",
    "mitsar_chls = [i.upper() for i in mitsar_chls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da834726-54e3-47a5-aee5-5559f2db6274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = TEST(splitted_paths)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "115f00e7-2dcb-4304-865f-57c352e7784f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mitsar_chls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfd4bd0a-a2f1-4c2d-802b-04863b0cd9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': tensor([[ 1.0932, -1.4590, -0.1665,  ..., -0.0149,  0.5969, -0.3100],\n",
       "         [ 1.2056, -1.3542, -0.0959,  ...,  0.1167,  0.9017, -0.1122],\n",
       "         [ 1.1443, -1.5456, -0.1252,  ...,  0.1326,  0.4841, -0.0782],\n",
       "         ...,\n",
       "         [ 0.8373,  0.9406,  0.3586,  ..., -0.2757,  1.3106,  0.1497],\n",
       "         [ 1.3198,  1.0934,  0.4207,  ..., -0.5446,  1.3738, -0.1641],\n",
       "         [ 1.3915,  1.2126,  0.4476,  ..., -0.6408,  1.0425, -0.3067]]),\n",
       " 'channels': tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "         19, 20, 21]),\n",
       " 'attention_mask': tensor([1., 1., 1.,  ..., 1., 1., 1.])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__getitem__(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a48eb7-0727-43ea-bedb-cd419ecd6532",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95ba1c89-cf1d-48d6-a6dd-4f8b0432ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c49688-e680-406f-9de5-2eae316eeb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50e2bc04-d8ab-4095-8081-c6e8d56fd6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class NoamLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, warmup_steps, d_model=512):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.d_model = d_model\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        last_epoch = max(1, self.last_epoch)\n",
    "        factor = min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n",
    "        # scale = self.warmup_steps ** 0.5 * min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n",
    "        # return [base_lr * scale for base_lr in self.base_lrs]\n",
    "        return [base_lr * self.d_model ** (-0.5) * factor for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cd39442-574d-489d-9cae-2886fa7b2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cossim = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "def cosloss(anchor, real, negative):\n",
    "    a = torch.exp(cossim(anchor, real)) / 0.1\n",
    "    b = sum([torch.exp(cossim(anchor, negative[:, n])) / 0.1 for n in range(negative.shape[1])]) + 1e-6\n",
    "    return -torch.log(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e28e456c-e80d-4cf6-ad0b-4c6af1427908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def worker_init_fn(worker_id):\n",
    "    torch_seed = torch.initial_seed()\n",
    "    random.seed(torch_seed + worker_id)\n",
    "    np.random.seed((torch_seed + worker_id) % 2**30)\n",
    "\n",
    "train_dataset = TEST(splitted_paths[:-15000])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, drop_last=True, worker_init_fn = worker_init_fn)\n",
    "\n",
    "    test_dataset = TEST(splitted_paths[-15000:])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, drop_last=True, worker_init_fn = worker_init_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a8e8d3c-6a8b-4af5-9d2d-98fb4eaa036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bc0dce6-c149-4043-bf79-d84a337177fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train()\n",
    "\n",
    "lr_d = 1\n",
    "acc_size = 8\n",
    "training_epochs1 = 100000 // len(train_loader)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr_d)\n",
    "\n",
    "model_test = torch.nn.DataParallel(model)\n",
    "model_test.to('cuda:0')\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "# scheduler = torch.optim.lr_scheduler.LinearLR(optim, start_factor=1.0, end_factor=0.1, total_iters=training_epochs1*len(train_loader))\n",
    "scheduler = NoamLR(optim, 100000, 512)\n",
    "\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25684f3b-1b46-41f3-ab64-342dd82ffb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2292, 43, 98556)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), training_epochs1, training_epochs1 * len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a8ee36c-93ab-408d-9171-951b2d01c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cpu()(batch['anchor'][None], batch['mask'][None], batch['channels'][None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301d68b-775a-47a1-a1a9-ff2ff4d20b3a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train\t12.755858182907104\n",
      "Loss/train\t12.643582820892334\n",
      "Loss/train\t12.454817771911621\n",
      "Loss/train\t12.185070753097534\n",
      "Loss/train\t11.838916063308716\n",
      "Loss: 1.477887749671936\t\n",
      "Loss/train\t11.409443974494934\n",
      "Loss/train\t10.90146255493164\n",
      "Loss/train\t10.320974111557007\n",
      "Loss/train\t9.67824900150299\n",
      "Loss/train\t8.992188572883606\n",
      "Loss: 1.1207029819488525\t\n",
      "Loss/train\t8.31241750717163\n",
      "Loss/train\t7.99466860294342\n",
      "Loss/train\t7.372100114822388\n",
      "Loss/train\t6.819593012332916\n",
      "Loss/train\t6.336482048034668\n",
      "Loss: 0.7635732293128967\t\n",
      "Loss/train\t5.912988305091858\n",
      "Loss/train\t5.541968882083893\n",
      "Loss/train\t5.225658714771271\n",
      "Loss/train\t4.952653110027313\n",
      "Loss/train\t4.719955623149872\n",
      "Loss: 0.576254665851593\t\n",
      "Loss/train\t4.516627132892609\n",
      "Loss/train\t4.346154153347015\n",
      "Loss/train\t4.199998915195465\n",
      "Loss/train\t4.136499226093292\n",
      "Loss/train\t4.017586529254913\n",
      "Loss/train\t3.9136194586753845\n",
      "Loss: 0.48888149857521057\t\n",
      "Loss/train\t3.8248919248580933\n",
      "Loss/train\t3.7474300265312195\n",
      "Loss/train\t3.6797587275505066\n",
      "Loss/train\t3.6180025935173035\n",
      "Loss/train\t3.5654504001140594\n",
      "Loss: 0.4454231560230255\t\n",
      "Loss/train\t3.517749160528183\n",
      "Loss/train\t3.477570593357086\n",
      "Loss/train\t3.4391528964042664\n",
      "Loss/train\t3.4062837064266205\n",
      "Loss/train\t3.3921675980091095\n",
      "Loss: 0.42217832803726196\t\n",
      "Loss/train\t3.365108996629715\n",
      "Loss/train\t3.339961439371109\n",
      "Loss/train\t3.319022983312607\n",
      "Loss/train\t3.299046367406845\n",
      "Loss/train\t3.2820617258548737\n",
      "Loss: 0.4091930687427521\t\n",
      "Loss/train\t3.2660966515541077\n",
      "Loss/train\t3.2521539628505707\n",
      "Loss/train\t3.238929182291031\n",
      "Loss/train\t3.2268972992897034\n",
      "Loss/train\t3.216195225715637\n",
      "Loss: 0.40142062306404114\t\n",
      "Loss/train\t3.2061351239681244\n",
      "Loss/train\t3.1923994719982147\n",
      "Loss/train\t3.1846416890621185\n",
      "Loss/train\t3.1765201687812805\n",
      "Loss/train\t3.1694579124450684\n",
      "Loss: 0.396173357963562\t\n",
      "Loss/train\t3.162583142518997\n",
      "Loss/train\t3.1557197868824005\n",
      "Loss/train\t3.1499793231487274\n",
      "Loss/train\t3.1444895267486572\n",
      "Loss/train\t3.138695240020752\n",
      "Loss: 0.3922988772392273\t\n",
      "Loss/train\t3.1322868168354034\n",
      "Loss/train\t3.1280214488506317\n",
      "Loss/train\t3.1260062754154205\n",
      "Loss/train\t3.1207349598407745\n",
      "Loss/train\t3.1165557503700256\n",
      "Loss: 0.38923749327659607\t\n",
      "Loss/train\t3.111479341983795\n",
      "Loss/train\t3.1068959534168243\n",
      "Loss/train\t3.1029442846775055\n",
      "Loss/train\t3.098528951406479\n",
      "Loss/train\t3.09549817442894\n",
      "Loss: 0.3866656422615051\t\n",
      "Loss/train\t3.092077612876892\n",
      "Loss/train\t3.0873886346817017\n",
      "Loss/train\t3.084429383277893\n",
      "Loss/train\t3.082130581140518\n",
      "Loss/train\t3.0791537761688232\n",
      "Loss/train\t3.075514942407608\n",
      "Loss: 0.38443100452423096\t\n",
      "Loss/train\t3.072321444749832\n",
      "Loss/train\t3.0686399936676025\n",
      "Loss/train\t3.0664722323417664\n",
      "Loss/train\t3.061935842037201\n",
      "Loss/train\t3.0598286986351013\n",
      "Loss: 0.3824310600757599\t\n",
      "Loss/train\t3.0558643341064453\n",
      "Loss/train\t3.0532078444957733\n",
      "Loss/train\t3.050230771303177\n",
      "Loss/train\t3.047433853149414\n",
      "Loss/train\t3.044699102640152\n",
      "Loss: 0.38062915205955505\t\n",
      "Loss/train\t3.043403059244156\n",
      "Loss/train\t3.0398872196674347\n",
      "Loss/train\t3.036885231733322\n",
      "Loss/train\t3.03528892993927\n",
      "Loss/train\t3.032084435224533\n",
      "Loss: 0.37899377942085266\t\n",
      "Loss/train\t3.031604826450348\n",
      "Loss/train\t3.0278061032295227\n",
      "Loss/train\t3.025331050157547\n",
      "Loss/train\t3.022030532360077\n",
      "Loss/train\t3.0207349956035614\n",
      "Loss: 0.37748050689697266\t\n",
      "Loss/train\t3.018094152212143\n",
      "Loss/train\t3.0176146924495697\n",
      "Loss/train\t3.0153778195381165\n",
      "Loss/train\t3.0125092566013336\n",
      "Loss/train\t3.009765535593033\n",
      "Loss/train\t3.0069188475608826\n",
      "Loss: 0.3760727643966675\t\n",
      "Loss/train\t3.006137728691101\n",
      "Loss/train\t3.0043415427207947\n",
      "Loss/train\t3.002405524253845\n",
      "Loss/train\t2.999369651079178\n",
      "Loss/train\t2.9971305429935455\n",
      "Loss: 0.37471261620521545\t\n",
      "Loss/train\t2.994408041238785\n",
      "Loss/train\t2.992760092020035\n",
      "Loss/train\t2.990832597017288\n",
      "Loss/train\t2.990477591753006\n",
      "Loss/train\t2.9877956807613373\n",
      "Loss: 0.3733205795288086\t\n",
      "Loss/train\t2.986118823289871\n",
      "Loss/train\t2.981015980243683\n",
      "Loss/train\t2.9802558720111847\n",
      "Loss/train\t2.9765120148658752\n",
      "Loss/train\t2.9765975773334503\n",
      "Loss: 0.3717987835407257\t\n",
      "Loss/train\t2.9724715650081635\n",
      "Loss/train\t2.9713196754455566\n",
      "Loss/train\t2.967473566532135\n",
      "Loss/train\t2.9643036127090454\n",
      "Loss/train\t2.9623642563819885\n",
      "Loss/train\t2.959965258836746\n",
      "Loss: 0.37001529335975647\t\n",
      "Loss/train\t2.956305652856827\n",
      "Loss/train\t2.9535030126571655\n",
      "Loss/train\t2.950118809938431\n",
      "Loss/train\t2.94686421751976\n",
      "Loss/train\t2.9447926580905914\n",
      "Loss: 0.36811453104019165\t\n",
      "Loss/train\t2.941439241170883\n",
      "Loss/train\t2.940099775791168\n",
      "Loss/train\t2.933786928653717\n",
      "Loss/train\t2.9334504902362823\n",
      "Loss/train\t2.9317177832126617\n",
      "Loss: 0.36634328961372375\t\n",
      "Loss/train\t2.928731918334961\n",
      "Loss/train\t2.925934761762619\n",
      "Loss/train\t2.924750953912735\n",
      "Loss/train\t2.922209084033966\n",
      "Loss/train\t2.918773740530014\n",
      "Loss: 0.36474496126174927\t\n",
      "Loss/train\t2.917858272790909\n",
      "Loss/train\t2.914899706840515\n",
      "Loss/train\t2.912205785512924\n",
      "Loss/train\t2.9107946157455444\n",
      "Loss/train\t2.9089353680610657\n",
      "Loss: 0.3633054494857788\t\n",
      "Loss/train\t2.9032803773880005\n",
      "Loss/train\t2.9044482707977295\n",
      "Loss/train\t2.901307851076126\n",
      "Loss/train\t2.9011556804180145\n",
      "Loss/train\t2.8967393338680267\n",
      "Loss/train\t2.8948283791542053\n",
      "Loss: 0.3619936406612396\t\n",
      "Loss/train\t2.893488496541977\n",
      "Loss/train\t2.8945826292037964\n",
      "Loss/train\t2.890445441007614\n",
      "Loss/train\t2.889322578907013\n",
      "Loss/train\t2.8862860202789307\n",
      "Loss: 0.36085671186447144\t\n",
      "Loss/train\t2.8846550583839417\n",
      "Loss/train\t2.8824138939380646\n",
      "Loss/train\t2.8805632889270782\n",
      "Loss/train\t2.877865046262741\n",
      "Loss: 0.3597930073738098\t\n",
      "Loss/train\t2.876994550228119\n",
      "Loss/train\t2.8748088777065277\n",
      "Loss/train\t2.8739466071128845\n",
      "Loss/train\t2.8730830550193787\n",
      "Loss/train\t2.8697808980941772\n",
      "Loss: 0.35880935192108154\t\n",
      "Loss/train\t2.8698234260082245\n",
      "Loss/train\t2.8656990826129913\n",
      "Loss/train\t2.866015374660492\n",
      "Loss/train\t2.8658622801303864\n",
      "Loss/train\t2.8644455671310425\n",
      "Loss: 0.35783228278160095\t\n",
      "Loss/train\t2.863614559173584\n",
      "Loss/train\t2.8591532707214355\n",
      "Loss/train\t2.856845945119858\n",
      "Loss/train\t2.855754554271698\n",
      "Loss/train\t2.854324847459793\n",
      "Loss: 0.35694777965545654\t\n",
      "Loss/train\t2.8533268868923187\n",
      "Loss/train\t2.854556620121002\n",
      "Loss/train\t2.8517045974731445\n",
      "Loss/train\t2.8503537476062775\n",
      "Loss/train\t2.848848044872284\n",
      "Loss: 0.35607069730758667\t\n",
      "Loss/train\t2.8469757437705994\n",
      "Loss/train\t2.845758020877838\n",
      "Loss/train\t2.8425829708576202\n",
      "Loss/train\t2.8419530391693115\n",
      "Loss: 0.355116069316864\t\n",
      "Loss/train\t2.8403874933719635\n",
      "Loss/train\t2.838493049144745\n",
      "Loss/train\t2.8370776772499084\n",
      "Loss/train\t2.837918758392334\n",
      "Loss/train\t2.832762748003006\n",
      "Loss: 0.35413530468940735\t\n",
      "Loss/train\t2.8325993716716766\n",
      "Loss/train\t2.8309223651885986\n",
      "Loss/train\t2.8307434916496277\n",
      "Loss/train\t2.8266052305698395\n",
      "Loss/train\t2.827120989561081\n",
      "Loss: 0.35319024324417114\t\n",
      "Loss/train\t2.823172479867935\n",
      "Loss/train\t2.824467718601227\n",
      "Loss/train\t2.820710629224777\n",
      "Loss/train\t2.819431245326996\n",
      "Loss/train\t2.8206507861614227\n",
      "Loss: 0.3523656725883484\t\n",
      "Loss/train\t2.819383680820465\n",
      "Loss/train\t2.817619800567627\n",
      "Loss/train\t2.814256966114044\n",
      "Loss/train\t2.814727395772934\n",
      "Loss: 0.3515779972076416\t\n",
      "Loss/train\t2.8120080530643463\n",
      "Loss/train\t2.810046136379242\n",
      "Loss/train\t2.810254007577896\n",
      "Loss/train\t2.8101383447647095\n",
      "Loss/train\t2.8087089359760284\n",
      "Loss: 0.3508453965187073\t\n",
      "Loss/train\t2.8048042356967926\n",
      "Loss/train\t2.8043673634529114\n",
      "Loss/train\t2.8037441074848175\n",
      "Loss/train\t2.8035619258880615\n",
      "Loss/train\t2.8002596497535706\n",
      "Loss: 0.35011374950408936\t\n",
      "Loss/train\t2.8011174499988556\n",
      "Loss/train\t2.8016227781772614\n",
      "Loss/train\t2.7980417907238007\n",
      "Loss/train\t2.79611673951149\n",
      "Loss/train\t2.794576644897461\n",
      "Loss: 0.34943118691444397\t\n",
      "Loss/train\t2.7949137091636658\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(training_epochs1):\n",
    "    mean_loss = 0\n",
    "    acc_step = 0\n",
    "    for batch in train_loader:\n",
    "        ae, label = model_test(\n",
    "            batch['anchor'],#.to('cuda:0'), \n",
    "            None, \n",
    "            batch['channels'].long())\n",
    "        \n",
    "        logits = _calculate_similarity(torch.transpose(label, 1, 2), torch.transpose(ae, 1, 2), _generate_negatives(torch.transpose(label, 1, 2))[0])\n",
    "\n",
    "        fake_labels = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits, fake_labels) + 0.001 * label.pow(2).mean()\n",
    "\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        mean_loss += loss.item()\n",
    "        acc_step += 1\n",
    "        steps += 1\n",
    "        # raise\n",
    "        if acc_step != 0 and acc_step % acc_size == 0:\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad()\n",
    "            if steps % 100 == 0:\n",
    "                print('Loss/train\\t{}'.format(mean_loss / acc_size))\n",
    "            writer.add_scalar('Loss/train', mean_loss / acc_size, steps)\n",
    "            mean_loss = 0\n",
    "        if steps != 0 and steps % 1000 == 0:\n",
    "            der = 0\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_loader:\n",
    "                        ae, label = model_test(\n",
    "                            batch['anchor'],#.to('cuda:0'), \n",
    "                            None, \n",
    "                            batch['channels'].long())\n",
    "                        logits = _calculate_similarity(torch.transpose(label, 1, 2), torch.transpose(ae, 1, 2), _generate_negatives(torch.transpose(label, 1, 2))[0])\n",
    "\n",
    "                        fake_labels = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)\n",
    "                        loss = torch.nn.CrossEntropyLoss()(logits, fake_labels) + 0.001 * label.pow(2).mean()\n",
    "\n",
    "                        loss = loss.mean() / acc_size\n",
    "                        der += loss\n",
    "                der /= len(test_loader)\n",
    "                writer.add_scalar('Loss/test', der, steps)\n",
    "\n",
    "                print('Loss: {}\\t'.format(der))\n",
    "            except:\n",
    "                raise\n",
    "            torch.save(model_test.module.state_dict(), 'models/step.pt'.format(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b119cee-d759-4fd8-a8bf-130e8b8d17d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ee4f4-4764-44d4-b27f-eb1da2804fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56e39d-e163-4e2f-a0d4-1825f5921796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728f769-bff6-4ecf-b750-a3e183a65366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0300b-9b95-4741-961f-f3b471ad7005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
