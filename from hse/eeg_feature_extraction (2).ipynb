{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fac45f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# MNE modules\n",
    "import mne\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "# Filter warnings\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mne.set_log_level(verbose='CRITICAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b4e27c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#----------------Architecture-------------------------------------------\n",
    "\n",
    "def init_chnls():\n",
    "    HSE_chls = ['Fp1', 'Fz', 'F3', 'F7', 'FC5', 'FC1', 'C3', 'T7',\n",
    "        'CP5', 'CP1', 'Pz', 'P3', 'P7', 'O1', 'Oz', 'O2', 'P4', 'P8', 'CP6',\n",
    "        'CP2', 'Cz', 'C4', 'T8', 'FC6', 'FC2', 'F4', 'F8', 'FP2']\n",
    "\n",
    "    HSE_chls = [i.upper() for i in HSE_chls]\n",
    "\n",
    "    mitsar_chls = ['Fp1', 'Fp2', 'FZ', 'FCz', 'Cz', 'Pz', 'O1', 'O2', 'F3', 'F4',\n",
    "                   'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', 'A1', 'A2']\n",
    "    mitsar_chls = [i.upper() for i in mitsar_chls]\n",
    "\n",
    "    raf_chls = ['Fp1', 'Fp2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'T3', 'C3', 'Cz',\n",
    "           'C4', 'T4', 'T5', 'P3', 'Pz', 'P4', 'T6', 'O1', 'O2', 'A1', 'A2']\n",
    "    raf_chls = [i.upper() for i in raf_chls]\n",
    "\n",
    "    m_channels = ['Fp1', 'FP2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'C3', 'Cz', 'C4', \\\n",
    "                  'P3', 'Pz', 'P4', 'O1', 'O2']\n",
    "    m_channels = [i.upper() for i in m_channels]\n",
    "    TUH_chanels = ['FP1', 'FP2', 'FZ', 'FCZ', 'CZ', 'PZ', 'O1', 'O2', 'F3', 'F4', \\\n",
    "                   'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', \\\n",
    "                   'A1', 'A2', 'EKG1']\n",
    "    TUH_chanels = [i.upper() for i in TUH_chanels]\n",
    "    #  Remove 'EKG1' and 'FCZ' based on pretrain dataloader TEST(): Pretraining.v5.ipynb\n",
    "    TUH_chanels_for_training = ['FP1', 'FP2', 'FZ', 'CZ', 'PZ', 'O1', 'O2', 'F3', 'F4', \\\n",
    "                                'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', \\\n",
    "                                'A1', 'A2']\n",
    "    TUH_chanels_for_training = [i.upper() for i in TUH_chanels_for_training]\n",
    "\n",
    "    #  TUH_chanels_for_training with  'FCZ' because model archetecture needs it in Pretraining.v5.ipynb\n",
    "    TUH_chanels_for_training_plus3 = ['FP1', 'FP2', 'FZ', 'FCz', 'CZ', 'PZ', 'O1', 'O2', 'F3', 'F4', \\\n",
    "                                      'F7', 'F8', 'C3', 'C4', 'T3', 'T4', 'P3', 'P4', 'T5', 'T6', \\\n",
    "                                      'A1', 'A2']\n",
    "    TUH_chanels_for_training_plus3 = [i.upper() for i in TUH_chanels_for_training_plus3]\n",
    "\n",
    "    HSE_Stage2_channels = ['Fp1', 'FP2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'C3', 'Cz', 'C4', \\\n",
    "                           'P3', 'Pz', 'P4', 'O1', 'O2']\n",
    "    HSE_Stage2_channels = [i.upper() for i in HSE_Stage2_channels]\n",
    "\n",
    "    # return HSE_chls, mitsar_chls, raf_chls\n",
    "    return HSE_chls, mitsar_chls, HSE_Stage2_channels, TUH_chanels, TUH_chanels_for_training, TUH_chanels_for_training_plus3\n",
    "\n",
    "\n",
    "HSE_chls, mitsar_chls, HSE_Stage2_channels, TUH_chanels, TUH_chanels_for_training, TUH_chanels_for_training_plus3  = init_chnls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "300cfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_bands():\n",
    "    # Frequency bands\n",
    "    bands = [(0.9, 4, 'Delta (0.9-4 Hz)', 'D'), (4, 8, 'Theta (4-8 Hz)', 'T'), (8, 14, 'Alpha (8-14 Hz)', 'A'),\n",
    "             (14, 25, 'Beta (14-25 Hz)', 'B'), (25, 40, 'Gamma (25-40 Hz)', 'G')]\n",
    "\n",
    "    str_freq = [bands[i][3] for i in range(len(bands))]\n",
    "\n",
    "    # Localization by scalp regions\n",
    "    regions = [(['Fp1', 'Fp2'], 'Fp', 'Pre-frontal'), (['F7', 'F3'], 'LF', 'Left Frontal'),\n",
    "               (['Fz'], 'MF', 'Midline Frontal'), (['F4', 'F8'], 'RF', 'Right Frontal'),\n",
    "               (['C3'], 'LT', 'Left Temporal'), (['P8'], 'RT', 'Right Temporal'),\n",
    "               (['C3', 'Cz', 'C4'], 'Cen', 'Central'), (['P3', 'Pz', 'P4'], 'Par', 'Parietal'),\n",
    "               (['O1', 'O2'], 'Occ', 'Occipital')]\n",
    "\n",
    "    n_freq = len(str_freq)\n",
    "    n_regions = len(regions)\n",
    "\n",
    "    return bands, str_freq, regions, n_freq, n_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6bfee4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sample, window=219, step=32, samp_rate=100):\n",
    "    sliced_data = []\n",
    "    slices_amount = int((sample.shape[0] - window) / step + 1)\n",
    "    for i in range(slices_amount):\n",
    "        slicee = sample[0 + i*step :window + i*step, :]\n",
    "        sliced_data.append(slicee)\n",
    "    \n",
    "    sliced_data = np.array(sliced_data) # events, chanels, window\n",
    "    sliced_data = sliced_data.reshape(slices_amount, sample.shape[1], window)\n",
    "\n",
    "    ch_names = HSE_Stage2_channels\n",
    "    n_channels = len(ch_names)\n",
    "    bands, str_freq, regions, n_freq, n_regions = define_bands()\n",
    "    \n",
    "    kwargs = dict(fmin=bands[0][0], fmax=bands[-1][1], sfreq=samp_rate, bandwidth=None, adaptive=True, n_jobs=1)\n",
    "    loc_masks = [[ch_names[i] in reg for i in range(n_channels)] for (reg, _, _) in regions]\n",
    "    \n",
    "    lst_st_psd_raw = []\n",
    "    lst_st_psd_loc_raw = []\n",
    "    lst_st_psd_all_raw = []\n",
    "    \n",
    "    st_psd_mtaper, st_freq_mtaper = psd_array_multitaper(sliced_data, **kwargs)\n",
    "    freq_masks = [(fmin < st_freq_mtaper) & (st_freq_mtaper < fmax) for (fmin, fmax, _, _) in bands]\n",
    "    \n",
    "\n",
    "    st_psd_raw = np.array([np.mean(st_psd_mtaper[:, :, _freq_mask], axis=2) for _freq_mask in freq_masks]).transpose(1, 2, 0)\n",
    "    st_psd_loc_raw = np.array([np.mean(st_psd_raw[:, _mask, :], axis=1) for _mask in loc_masks]).transpose(1, 0, 2)\n",
    "    st_psd_all_raw = np.mean(st_psd_raw, axis=1)\n",
    "\n",
    "    df_st_raw = pd.DataFrame()\n",
    "    for _fr in range(n_freq):\n",
    "        for _ch in range(n_channels):\n",
    "            df_st_raw[str_freq[_fr] + '_psd_' + ch_names[_ch]] = st_psd_raw[:, _ch, _fr]\n",
    "    \n",
    "    \n",
    "    df = df_st_raw\n",
    "\n",
    "    lst_st = 10 * np.log10(df) #- 10 * np.log10(df_blm_psd_raw.mean(axis=0))\n",
    "    # need to subtract baseline!!!\n",
    "    # so need to have baseline file\n",
    "    \n",
    "    return lst_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "4634b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_indices(psd_previous):\n",
    "    samp_rate = 100\n",
    "    ch_names = HSE_Stage2_channels\n",
    "    n_channels = len(ch_names)\n",
    "    \n",
    "    bands, str_freq, regions, n_freq, n_regions = define_bands()\n",
    "\n",
    "    # PSD special features (EEG indices) (re-referenced data)\n",
    "\n",
    "    lst_st_psd_ind_raw = []\n",
    "    lst_st_psd_ind_loc_raw = []\n",
    "    lst_st_psd_ind_all_raw = []\n",
    "\n",
    "    str_psd_ind = ['T_D', 'A_D', 'A_T', 'A_DT', 'B_D', 'B_T', 'B_A', 'B_DT', 'B_TA', 'G_D', 'G_T', 'G_A', 'G_B', 'G_DT',\n",
    "                   'G_TA', 'G_AB']\n",
    "\n",
    "    df_st_raw = pd.DataFrame()\n",
    "    df_st_loc_raw = pd.DataFrame()\n",
    "    df_st_all_raw = pd.DataFrame()\n",
    "\n",
    "    # Indices per channel (averaged PSD)\n",
    "    for _ch in range(n_channels):\n",
    "        for ind in str_psd_ind:\n",
    "            if (len(ind) == 3):\n",
    "                df_st_raw[ind + '_psd_' + ch_names[_ch]] = (psd_previous[ind[0] + '_psd_' + ch_names[_ch]] /\n",
    "                                                            psd_previous[ind[2] + '_psd_' + ch_names[_ch]])\n",
    "            elif (len(ind) == 4):\n",
    "                df_st_raw[ind + '_psd_' + ch_names[_ch]] = (psd_previous[ind[0] + '_psd_' + ch_names[_ch]] /\n",
    "                                                            psd_previous[ind[2] + '_psd_' + ch_names[_ch]] +\n",
    "                                                             psd_previous[ind[3] + '_psd_' + ch_names[_ch]])\n",
    "    lst_st_psd_ind_raw = df_st_raw\n",
    "    lst_st_psd_ind_loc_raw = df_st_loc_raw\n",
    "    lst_st_psd_ind_all_raw = df_st_all_raw\n",
    "\n",
    "    # Aggregate all stages in one DataFrame\n",
    "    df = lst_st_psd_ind_raw\n",
    "    lst_st = 10 * np.log10(df)\n",
    "\n",
    "    return lst_st\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1d52bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEST_TUH(torch.utils.data.Dataset):\n",
    "    def __init__(self, path): #, tuh_filtered_stat_vals):\n",
    "        super(TEST_TUH, self).__init__()\n",
    "        self.main_path = path\n",
    "        self.paths = path\n",
    "        # print(self.paths)\n",
    "        # self.tuh_filtered_stat_vals = tuh_filtered_stat_vals\n",
    "        # self.paths = ['{}/{}'.format(self.main_path, i) for i in os.listdir(self.main_path)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx, negative=False):\n",
    "        path = self.paths[idx]\n",
    "        # take 60s of recording with specified shift\n",
    "        key = False\n",
    "        while (key == False):\n",
    "            try:\n",
    "                # sample = np.load(path, allow_pickle=True).item()['value']\n",
    "                sample = np.load(path, allow_pickle=True).item()\n",
    "                key = True\n",
    "            except Exception as e:\n",
    "                print(\"Path: {} is broken \".format(path), e)\n",
    "                path = np.random.choice(self.paths, 1)[0]\n",
    "                # sample = np.load(path, allow_pickle=True).item()['value']\n",
    "        real_len = min(3000, sample['value_pure'].shape[0])\n",
    "        channels_ids = [i for i, val in enumerate(sample['channels']) if val in HSE_Stage2_channels]\n",
    "\n",
    "        sample = sample['value_pure'][:real_len]\n",
    "\n",
    "        # choose 2 random channels\n",
    "        channels_to_train = channels_ids  # np.random.choice(channels_ids, 2, replace=False)\n",
    "        channels_vector = torch.tensor((channels_to_train))\n",
    "        sample = sample[:, channels_to_train]\n",
    "\n",
    "        sample_norm = sample\n",
    "        if sample_norm.shape[0] < 3000:\n",
    "            sample_norm = np.pad(sample_norm, ((0, 3000 - sample_norm.shape[0]), (0, 0)))\n",
    "        lst_st_feat = extract_features(sample_norm)\n",
    "        indices = features_indices(lst_st_feat)\n",
    "        \n",
    "        df_st_eeg = pd.concat([lst_st_feat, indices], axis=1).dropna()\n",
    "        print(f'EEG features total: {df_st_eeg.shape}')\n",
    "        print()\n",
    "        class_from_clusterisation = torch.ones(df_st_eeg.shape[0])\n",
    "\n",
    "        attention_mask = torch.ones(3000)\n",
    "        attention_mask[real_len:] = 0\n",
    "\n",
    "        if np.random.choice([0, 1], p=[0.7, 0.3]) and not negative:\n",
    "            index = np.random.choice(self.__len__() - 1)\n",
    "            negative_sample = self.__getitem__(index, True)\n",
    "            negative_path = negative_sample['path']\n",
    "            negative_sample_norm = negative_sample['current'].numpy()\n",
    "            df_st_eeg_negative = negative_sample['features']\n",
    "            class_from_clusterisation_negative = negative_sample['classes']\n",
    "\n",
    "\n",
    "            negative_person = negative_sample['path'].split('/')[-1]  # .split('_')\n",
    "            current_person = path.split('/')[-1]  # .split('_')\n",
    "            if negative_person.split('_')[0] == current_person.split('_')[0] and \\\n",
    "                    abs(int(negative_person.split('_')[1][:-4]) - int(current_person.split('_')[1][:-4])) < 20000:\n",
    "                negative_label = torch.tensor(0)               # возможно стоит запретить позитивы отличающиеся < 20000 , если состояние реально изменилось то сеть будет учиться странному.\n",
    "            else:\n",
    "                negative_label = torch.tensor(1)\n",
    "\n",
    "            return {'current': torch.from_numpy(sample_norm).float(),\n",
    "                    'negative': torch.from_numpy(negative_sample_norm).float(),\n",
    "                    'path': path,\n",
    "                    'label': negative_label,\n",
    "                    'channels': channels_vector,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'features': torch.from_numpy(df_st_eeg.to_numpy()).float(),\n",
    "                    'features_negative' : df_st_eeg_negative,\n",
    "                    'classes': class_from_clusterisation,\n",
    "                    'classes_negative': class_from_clusterisation_negative}\n",
    "        else:\n",
    "            negative_sample_norm = sample_norm.copy()\n",
    "            df_st_eeg_negative = df_st_eeg.copy()\n",
    "            class_from_clusterisation_negative = class_from_clusterisation\n",
    "            negative_label = torch.tensor(0)\n",
    "            negative_path = ''\n",
    "\n",
    "\n",
    "            return {'current': torch.from_numpy(sample_norm).float(),\n",
    "                    'negative': torch.from_numpy(negative_sample_norm).float(),\n",
    "                    'path': path,\n",
    "                    'label': negative_label,\n",
    "                    'channels': channels_vector,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'features': torch.from_numpy(df_st_eeg.to_numpy()).float(),\n",
    "                    'features_negative' : torch.from_numpy(df_st_eeg_negative.to_numpy()).float(),\n",
    "                    'classes': class_from_clusterisation,\n",
    "                    'classes_negative': class_from_clusterisation_negative}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "196fd73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/example_data/TUH/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2bf9b155",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_paths = [f'{path}/{i}'.format(i) for i in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "922af638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TEST_TUH(splitted_paths[1:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c7262e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "27c40475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (87, 240)\n",
      "EEG features total: (87, 315)\n",
      "\n",
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (87, 240)\n",
      "EEG features total: (87, 315)\n",
      "\n",
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (87, 240)\n",
      "EEG features total: (87, 315)\n",
      "\n",
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (28, 240)\n",
      "EEG features total: (28, 315)\n",
      "\n",
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (87, 240)\n",
      "EEG features total: (87, 315)\n",
      "\n",
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (87, 240)\n",
      "EEG features total: (87, 315)\n",
      "\n",
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (86, 240)\n",
      "EEG features total: (86, 315)\n",
      "\n",
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (87, 240)\n",
      "EEG features total: (87, 315)\n",
      "\n",
      "lst_st_feat with DROP: (87, 75)\n",
      "indices with DROP: (28, 240)\n",
      "EEG features total: (28, 315)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    features = batch['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6be4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
